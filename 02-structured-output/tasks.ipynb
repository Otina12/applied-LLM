{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "35ef32c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1032abb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import importlib.util\n",
        "from typing import List\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6999e295-6e3d-48cb-a644-527f68a98b27",
      "metadata": {
        "id": "6999e295-6e3d-48cb-a644-527f68a98b27"
      },
      "source": [
        "### Applied LLM Systems - Lab Exercise 5: Dynamic Schema Generation\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "\n",
        "Build a system that learns the structure of data by analyzing examples, then applies that learned structure to extract information from new text. This demonstrates adaptive schema generation based on content analysis.\n",
        "\n",
        "## ðŸ“‹ Task Description\n",
        "\n",
        "You are given three text files:\n",
        "\n",
        "* `example1.txt` - First example text about cities\n",
        "* `example2.txt` - Second example text about cities\n",
        "* `target.txt` - New text to extract information from\n",
        "\n",
        "Your system should:\n",
        "\n",
        "1. Analyze the two example texts to identify what information they commonly contain\n",
        "2. Generate a Pydantic schema based on discovered common fields\n",
        "3. Use that schema to extract structured data from the target text\n",
        "\n",
        "## Step 1: Schema Discovery\n",
        "\n",
        "Ask your LLM to analyze example1.txt and example2.txt to identify:\n",
        "\n",
        "* What fields/attributes are present in both texts\n",
        "* What data types each field should have\n",
        "* Which fields should be required vs. optional\n",
        "\n",
        "The LLM should output a description or actual Pydantic model definition.\n",
        "\n",
        "## Step 2: Generate Pydantic Model\n",
        "\n",
        "Either:\n",
        "\n",
        "* Have the LLM generate the actual Python code for the Pydantic model, or\n",
        "* Take the LLM's field descriptions and write the model yourself\n",
        "\n",
        "Save this as `schema.py`\n",
        "\n",
        "## Step 3: Extract from Target\n",
        "\n",
        "Use the generated schema with structured output to extract information from `target.txt`.\n",
        "\n",
        "Save the result as `extracted_data.json`\n",
        "\n",
        "\n",
        "## Example inputs:\n",
        "\n",
        "### example1.txt:\n",
        "```Barcelona is located in Spain on the Mediterranean coast. It has a population of 1.6 million and is known for Sagrada Familia and Park GÃ¼ell. The official languages are Spanish and Catalan. Summer temperatures average 28Â°C.```\n",
        "\n",
        "### example2.txt:\n",
        "```Amsterdam is the capital of the Netherlands with 872,000 inhabitants. Famous landmarks include the Anne Frank House and Van Gogh Museum. Dutch is the official language, and the average summer temperature is around 22Â°C.```\n",
        "\n",
        "### target.txt:\n",
        "```Lisbon is Portugal's coastal capital with a population of 505,000 people. Portuguese is the official language. The city is famous for BelÃ©m Tower and JerÃ³nimos Monastery. Summer temperatures typically reach 27Â°C.```\n",
        "# ðŸ”§ Your Workflow\n",
        "\n",
        "## Step 1: Schema Discovery\n",
        "\n",
        "Ask your LLM to analyze example1.txt and example2.txt to identify:\n",
        "\n",
        "* What fields/attributes are present in both texts\n",
        "* What data types each field should have\n",
        "* Which fields should be required vs. optional\n",
        "\n",
        "The LLM should output a description or actual Pydantic model definition.\n",
        "\n",
        "## Step 2: Generate Pydantic Model\n",
        "\n",
        "Either:\n",
        "\n",
        "* Have the LLM generate the actual Python code for the Pydantic model, or\n",
        "* Take the LLM's field descriptions and write the model yourself\n",
        "\n",
        "Save this as `schema.py`\n",
        "\n",
        "## Step 3: Extract from Target\n",
        "\n",
        "Use the generated schema with structured output to extract information from `target.txt`.\n",
        "\n",
        "Save the result as `extracted_data.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80f2c931-0b2d-4566-a2df-2d564be47c78",
      "metadata": {
        "id": "80f2c931-0b2d-4566-a2df-2d564be47c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote assets/task1/out/schema.py\n",
            "Wrote assets/task1/out/extracted_data.json\n"
          ]
        }
      ],
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "task1_path = \"assets/task1/\"\n",
        "input_path = task1_path + \"in/\"\n",
        "output_path = task1_path + \"out/\"\n",
        "\n",
        "example1_path = input_path + \"example1.txt\"\n",
        "example2_path = input_path + \"example2.txt\"\n",
        "target_path = input_path + \"target.txt\"\n",
        "py_schema_path = output_path + \"schema.py\"\n",
        "output_json_path = output_path + \"extracted_data.json\"\n",
        "\n",
        "\n",
        "def read_text(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def extract_code_block(text, language_hint=\"python\"):\n",
        "    pattern_lang = re.compile(rf\"```{language_hint}\\s*(.*?)```\", re.DOTALL | re.IGNORECASE)\n",
        "    m = pattern_lang.search(text)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "\n",
        "    pattern_any = re.compile(r\"```\\s*(.*?)```\", re.DOTALL)\n",
        "    m = pattern_any.search(text)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "\n",
        "    return None\n",
        "\n",
        "def write_schema_py(code):\n",
        "    with open(py_schema_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(code)\n",
        "    print(f\"Wrote {py_schema_path}\")\n",
        "\n",
        "def import_pydantic_model(module_path):\n",
        "    spec = importlib.util.spec_from_file_location(\"schema\", module_path)\n",
        "    if not spec or not spec.loader:\n",
        "        raise RuntimeError(\"Could not load schema.py\")\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "\n",
        "    if hasattr(module, \"CityInfo\"):\n",
        "        cls = getattr(module, \"CityInfo\")\n",
        "        if isinstance(cls, type) and issubclass(cls, BaseModel):\n",
        "            return cls\n",
        "\n",
        "    for name, obj in module.__dict__.items():\n",
        "        if isinstance(obj, type) and issubclass(obj, BaseModel) and obj is not BaseModel:\n",
        "            return obj\n",
        "\n",
        "    raise RuntimeError(\"No Pydantic BaseModel found in schema.py\")\n",
        "\n",
        "def get_json_schema_from_model(model_cls):\n",
        "    try:\n",
        "        return model_cls.model_json_schema()\n",
        "    except Exception:\n",
        "        return model_cls.schema()\n",
        "\n",
        "def discover_schema_from_examples(ex1, ex2):\n",
        "    system = (\n",
        "        \"You write clean Pydantic v2 models. \"\n",
        "        \"Produce one BaseModel named CityInfo that captures only fields present in both example texts. \"\n",
        "        \"Use simple types such as str, int, float, list[str]. \"\n",
        "        \"Mark a field required only if it appears clearly in both examples. \"\n",
        "        \"Return a single python fenced block with only the model code.\"\n",
        "    )\n",
        "\n",
        "    user = f\"\"\"\n",
        "Two city example texts follow.\n",
        "\n",
        "Example A:\n",
        "{ex1}\n",
        "\n",
        "Example B:\n",
        "{ex2}\n",
        "\n",
        "Task:\n",
        "1) Identify fields present in both texts.\n",
        "2) Propose types for each field.\n",
        "3) Output a Pydantic v2 model named CityInfo with those fields and short docstrings.\n",
        "Return the model inside one python fenced block. No extra talk.\n",
        "\"\"\"\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    content = resp.choices[0].message.content or \"\"\n",
        "    code = extract_code_block(content, \"python\")\n",
        "\n",
        "    if not code:\n",
        "        raise RuntimeError(\"The model did not return a python code block with the schema\")\n",
        "    return code\n",
        "\n",
        "def extract_with_schema(model_cls, target_text):\n",
        "    json_schema = get_json_schema_from_model(model_cls)\n",
        "\n",
        "    system = (\n",
        "        \"You extract structured data in JSON. \"\n",
        "        \"You must produce JSON that validates against the provided JSON schema. \"\n",
        "        \"Only output the JSON.\"\n",
        "    )\n",
        "\n",
        "    user_payload = {\n",
        "        \"task\": \"Extract data from the given text following the JSON schema\",\n",
        "        \"text\": target_text,\n",
        "        \"json_schema\": json_schema,\n",
        "        \"hints\": [\n",
        "            \"Population is a number of people. Use int if the text gives a whole number, else use float.\",\n",
        "            \"If languages are multiple, return a list of strings.\",\n",
        "            \"Temperatures are in Celsius as numbers where possible.\",\n",
        "            \"Landmarks should be a list of strings when present.\"\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(user_payload, ensure_ascii=False)},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    content = resp.choices[0].message.content or \"{}\"\n",
        "    data = json.loads(content)\n",
        "\n",
        "    try:\n",
        "        obj = model_cls(**data)\n",
        "    except ValidationError as ve:\n",
        "        raise SystemExit(f\"Validation failed.\\n{ve}\")\n",
        "    return obj.model_dump()\n",
        "\n",
        "ex1 = read_text(example1_path)\n",
        "ex2 = read_text(example2_path)\n",
        "\n",
        "code = discover_schema_from_examples(ex1, ex2)\n",
        "write_schema_py(code)\n",
        "\n",
        "model_cls = import_pydantic_model(py_schema_path)\n",
        "\n",
        "target_text = read_text(target_path)\n",
        "result = extract_with_schema(model_cls, target_text)\n",
        "\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Wrote {output_json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c72adf3-b73b-4cb0-92c7-51f4e22223a9",
      "metadata": {
        "id": "2c72adf3-b73b-4cb0-92c7-51f4e22223a9"
      },
      "source": [
        "# ðŸ“ The Task: Analyzing a GitHub Repository\n",
        "\n",
        "Your goal is to create a function, `investigate_github_repo(user_query: str)`, that can take a natural language query like \"Tell me about the repository for the Python library 'requests'\" and provide a detailed summary.\n",
        "\n",
        "## Step 1: Define the Pydantic Tool Schema\n",
        "\n",
        "Students must define a Pydantic model that represents the required arguments for the GitHub API call.\n",
        "```python\n",
        "GitHubRepoQuery(BaseModel):\n",
        "    owner (str): The username of the repository owner (e.g., \"psf\").\n",
        "    repo (str): The name of the repository (e.g., \"requests\").\n",
        "```\n",
        "\n",
        "## Step 2: Implement the Tool Generation Logic\n",
        "\n",
        "Create a function, `get_api_args(user_query: str)`, which does the following:\n",
        "\n",
        "* Uses a strong System Prompt to instruct the LLM to act as a GitHub Argument Generator.\n",
        "* Uses the `GitHubRepoQuery` model with the OpenAI API's structured output feature to generate the correct `owner` and `repo` names from the `user_query`.\n",
        "* The function should return a populated `GitHubRepoQuery` Pydantic object (e.g., `GitHubRepoQuery(owner=\"psf\", repo=\"requests\")`).\n",
        "\n",
        "## Step 3: Implement the External Tool (The API Call)\n",
        "\n",
        "Create a function, `call_github_api(query_args: GitHubRepoQuery)`, which simulates the external tool execution:\n",
        "\n",
        "* Constructs the correct GitHub REST API URL: `https://api.github.com/repos/{owner}/{repo}`.\n",
        "* Uses the `requests` library to make a `GET` request.\n",
        "* Crucially: Filters the massive JSON response from GitHub to extract only the most relevant fields that the LLM will need to answer the final question (e.g., `name`, `description`, `stargazers_count`, `forks_count`, `language`, `open_issues_count`, `html_url`).\n",
        "* Returns the filtered data (a smaller Python dictionary/JSON object).\n",
        "\n",
        "## Step 4: Define the Final Answer Schema & Interpretation\n",
        "\n",
        "To ensure the final output is also structured, define a final Pydantic model for the answer.\n",
        "```python\n",
        "RepoSummary(BaseModel):\n",
        "    summary_title (str): A creative, descriptive title for the summary.\n",
        "    key_stats (List[str]): A list of key facts about the repository (e.g., \"It has 50k stars.\").\n",
        "    llm_analysis (str): A final paragraph summarizing the project's purpose and status based on the data.\n",
        "    github_url (str): The direct link to the repository.\n",
        "```\n",
        "Create the final generation function, `get_final_summary(query: str, repo_data: dict)`:\n",
        "\n",
        "* Uses a new strong System Prompt to instruct the LLM to act as an Expert Data Analyst.\n",
        "* The User Prompt should be a combination of the original `user_query` and the retrieved `repo_data` (the filtered JSON from Step 3).\n",
        "* Uses the `RepoSummary` model for the final structured output.\n",
        "\n",
        "## Step 5: The Main Execution Flow\n",
        "\n",
        "Combine all steps into the main `investigate_github_repo` function:\n",
        "```python\n",
        "def investigate_github_repo(user_query: str) -> RepoSummary:\n",
        "    # 1. Generate API Arguments\n",
        "    api_args = get_api_args(user_query)\n",
        "\n",
        "    # 2. Call the External Tool (GitHub API)\n",
        "    repo_data = call_github_api(api_args)\n",
        "\n",
        "    # 3. Interpret the Results and Generate Final Answer\n",
        "    final_summary = get_final_summary(user_query, repo_data)\n",
        "\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4d0f529f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from pydantic import Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6d20a3dc-21ec-45cc-9822-753bb75d0b2c",
      "metadata": {
        "id": "6d20a3dc-21ec-45cc-9822-753bb75d0b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"summary_title\": \"Raft Consensus Algorithm in Go\",\n",
            "  \"key_stats\": [\n",
            "    \"0 stars\",\n",
            "    \"0 forks\",\n",
            "    \"0 open issues\",\n",
            "    \"Created on August 26, 2025\",\n",
            "    \"Last updated on September 21, 2025\"\n",
            "  ],\n",
            "  \"llm_analysis\": \"The 'raft-go' project is an implementation of the Raft consensus algorithm written in Go. It is likely aimed at developers and engineers interested in distributed systems and consensus algorithms. The lack of stars, forks, and open issues suggests that the project is new and may still be in the early stages of development or has not yet gained traction in the community.\",\n",
            "  \"github_url\": \"https://github.com/Otina12/raft-go\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai_model = \"gpt-4o-mini\"\n",
        "client = OpenAI(api_key = openai_api_key)\n",
        "\n",
        "class GitHubRepoQuery(BaseModel):\n",
        "    owner: str = Field(..., description = \"Owner of the repo\")\n",
        "    repo: str  = Field(..., description = \"The repository name\")\n",
        "\n",
        "class RepoSummary(BaseModel):\n",
        "    summary_title: str\n",
        "    key_stats: List[str]\n",
        "    llm_analysis: str\n",
        "    github_url: str\n",
        "\n",
        "def get_api_args(user_prompt) -> GitHubRepoQuery:\n",
        "    system = (\n",
        "        \"You are a GitHub argument generator. \"\n",
        "        \"Read the user query and infer the correct GitHub repository owner and repository name. \"\n",
        "        \"If the query says Python library requests, you should return owner and repo requests. \"\n",
        "        \"Respond using the provided Pydantic schema only.\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.parse(\n",
        "        model = openai_model,\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        response_format = GitHubRepoQuery,\n",
        "        temperature = 0.0,\n",
        "    )\n",
        "\n",
        "    args_obj = resp.choices[0].message.parsed\n",
        "    if not isinstance(args_obj, GitHubRepoQuery):\n",
        "        raise RuntimeError(\"Failed to parse API arguments\")\n",
        "    return args_obj\n",
        "\n",
        "def call_github_api(repo_info_query):\n",
        "    owner = repo_info_query.owner\n",
        "    repo = repo_info_query.repo\n",
        "\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"User-Agent\": \"openai-structured-output-demo\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, headers = headers, timeout = 15)\n",
        "    except requests.RequestException as ex:\n",
        "        raise SystemExit(f\"Network error when calling GitHub. Details: {ex}\")\n",
        "\n",
        "    if r.status_code == 404:\n",
        "        raise SystemExit(f\"Repository not found at {url}\")\n",
        "    if r.status_code >= 400:\n",
        "        raise SystemExit(f\"GitHub API error {r.status_code}. Body: {r.text}\")\n",
        "\n",
        "    data = r.json()\n",
        "\n",
        "    filtered = {\n",
        "        \"name\": data.get(\"name\"),\n",
        "        \"full_name\": data.get(\"full_name\"),\n",
        "        \"description\": data.get(\"description\"),\n",
        "        \"stargazers_count\": data.get(\"stargazers_count\"),\n",
        "        \"forks_count\": data.get(\"forks_count\"),\n",
        "        \"language\": data.get(\"language\"),\n",
        "        \"open_issues_count\": data.get(\"open_issues_count\"),\n",
        "        \"watchers_count\": data.get(\"subscribers_count\") or data.get(\"watchers_count\"),\n",
        "        \"license\": (data.get(\"license\") or {}).get(\"spdx_id\") if data.get(\"license\") else None,\n",
        "        \"topics\": data.get(\"topics\", []),\n",
        "        \"html_url\": data.get(\"html_url\"),\n",
        "        \"default_branch\": data.get(\"default_branch\"),\n",
        "        \"archived\": data.get(\"archived\"),\n",
        "        \"disabled\": data.get(\"disabled\"),\n",
        "        \"created_at\": data.get(\"created_at\"),\n",
        "        \"updated_at\": data.get(\"updated_at\"),\n",
        "        \"pushed_at\": data.get(\"pushed_at\"),\n",
        "    }\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def get_final_summary(user_prompt, repo_data):\n",
        "    system = (\n",
        "        \"You are an Expert Data Analyst. \"\n",
        "        \"Write a clear and helpful summary of a GitHub repository using the provided data. \"\n",
        "        \"The title should be short and descriptive. \"\n",
        "        \"key_stats should be a list of punchy facts drawn from the data. \"\n",
        "        \"llm_analysis should be a concise paragraph that explains what the project is, \"\n",
        "        \"who might use it, and what the activity level suggests. \"\n",
        "        \"Return only the structured fields defined by the schema.\"\n",
        "    )\n",
        "\n",
        "    user = {\n",
        "        \"original_query\": user_prompt,\n",
        "        \"repo_data\": repo_data,\n",
        "    }\n",
        "\n",
        "    resp = client.chat.completions.parse(\n",
        "        model = openai_model,\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(user, ensure_ascii = False)},\n",
        "        ],\n",
        "        response_format = RepoSummary,\n",
        "        temperature = 0.2,\n",
        "    )\n",
        "\n",
        "    summary_obj = resp.choices[0].message.parsed\n",
        "    if not isinstance(summary_obj, RepoSummary):\n",
        "        raise RuntimeError(\"Failed to parse final summary\")\n",
        "    return summary_obj\n",
        "\n",
        "def investigate_github_repo(user_query):\n",
        "    api_args = get_api_args(user_query)\n",
        "    repo_data = call_github_api(api_args)\n",
        "    final_summary = get_final_summary(user_query, repo_data)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "\n",
        "# query = input(\"Describe the github repo you want to analyze:\\n\")\n",
        "query = \"Analyze my repo. My username is Otina12, the repo is called raft-go\"\n",
        "result = investigate_github_repo(query)\n",
        "print(result.model_dump_json(indent = 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e401634c-2be3-4f6c-bc07-9455c766966d",
      "metadata": {
        "id": "e401634c-2be3-4f6c-bc07-9455c766966d"
      },
      "source": [
        "## ðŸ“œ Scenario\n",
        "\n",
        "You are tasked with building the backend system for a new Music Q&A Service. This service takes a natural language query from a user and must provide an accurate, synthesized answer based on factual data retrieved from the Spotify Web API.\n",
        "\n",
        "The core difficulty is designing a system that can intelligently determine what kind of data is needed and which API call to execute before presenting a final, conversational answer.\n",
        "\n",
        "## ðŸŽµ Available Spotify API Endpoints\n",
        "\n",
        "Your system is limited to using the following search capabilities. Your code must dynamically decide which type parameter to use based on the LLM's structured instruction.\n",
        "\n",
        "| API Function | Search Type (type parameter) | Key Data Points in Response |\n",
        "|--------------|------------------------------|----------------------------|\n",
        "| Search for Artists | artist | ID (essential), Followers Count, Genres, Images, Popularity. |\n",
        "| Search for Albums | album | ID (essential), Album Name, Release Date, Artists, Track Count. |\n",
        "\n",
        "### API Access Requirements:\n",
        "\n",
        "* **Base URL:** All search requests use the `/v1/search` endpoint.\n",
        "* **Authentication:** Requests must include the Access Token in the header: `Authorization: Bearer <Your_Access_Token>`\n",
        "* **Query Parameter:** The `q` parameter holds the search term (e.g., `q=Metallica` or `q=Master of Puppets`).\n",
        "\n",
        "## ðŸ“ˆ Required System Workflow\n",
        "\n",
        "Your application must implement a single main entry point that executes the following logical flow:\n",
        "\n",
        "1. **Route Query:** Call the LLM using your Tool Request Schema to convert the user's query into structured API parameters.\n",
        "2. **Execute API:** Use the parameters generated in Step 2 to construct and execute the correct Spotify search request (type=artist OR type=album).\n",
        "3. **Retrieve Data:** Capture the complete JSON response from Spotify.\n",
        "4. **Synthesize Answer:** Call the LLM a second time. Provide the original user query and the retrieved raw JSON data. Instruct the LLM to provide a final, conversational, and accurate answer to the user based only on the data you supply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "94c553b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ba485387",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"summary_title\": \"Is This It by The Strokes\",\n",
            "  \"key_points\": [\n",
            "    \"Album Name: Is This It\",\n",
            "    \"Release Date: July 30, 2001\",\n",
            "    \"Total Tracks: 11\",\n",
            "    \"Artist: The Strokes\",\n",
            "    \"Spotify Link: https://open.spotify.com/album/2yNaksHgeMQM9Quse463b5\"\n",
            "  ],\n",
            "  \"answer_text\": \"The album \\\"Is This It\\\" by The Strokes was released on July 30, 2001. It contains 11 tracks and is available on Spotify [here](https://open.spotify.com/album/2yNaksHgeMQM9Quse463b5).\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "spotify_access_token = os.getenv(\"SPOTIFY_ACCESS_TOKEN\")\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "class SpotifySearchArgs(BaseModel):\n",
        "    search_type: Literal[\"artist\", \"album\"] = Field(...)\n",
        "    query: str = Field(...)\n",
        "    reasoning: Optional[str] = Field(None)\n",
        "\n",
        "class MusicAnswer(BaseModel):\n",
        "    summary_title: str\n",
        "    key_points: List[str]\n",
        "    answer_text: str\n",
        "\n",
        "def get_api_args(user_query):\n",
        "    system = (\n",
        "        \"You are a Spotify Argument Generator. \"\n",
        "        \"You do not know upfront which API to use. \"\n",
        "        \"Investigate the user request and decide whether the task needs artist data or album data. \"\n",
        "        \"Return structured arguments using the schema. \"\n",
        "        \"Pick search_type as artist or album. \"\n",
        "        \"Set query to the search string that Spotify would expect in the q parameter. \"\n",
        "        \"Examples. If the user asks about Metallica the band, choose artist. \"\n",
        "        \"If the user asks about Master of Puppets the album, choose album.\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.parse(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user_query},\n",
        "        ],\n",
        "        response_format=SpotifySearchArgs,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    args_obj = resp.choices[0].message.parsed\n",
        "    if not isinstance(args_obj, SpotifySearchArgs):\n",
        "        raise RuntimeError(\"Could not parse Spotify arguments\")\n",
        "    return args_obj\n",
        "\n",
        "def call_spotify_api(query_args):\n",
        "    base_url = \"https://api.spotify.com/v1/search\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {spotify_access_token}\",\n",
        "        \"Accept\": \"application/json\",\n",
        "    }\n",
        "    params = {\n",
        "        \"q\": query_args.query,\n",
        "        \"type\": query_args.search_type,\n",
        "        \"limit\": 5,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(base_url, headers=headers, params=params, timeout=15)\n",
        "    except requests.RequestException as ex:\n",
        "        raise SystemExit(f\"Network error when calling Spotify. Details: {ex}\")\n",
        "\n",
        "    if r.status_code == 401:\n",
        "        raise SystemExit(\"Spotify says unauthorized. Check SPOTIFY_ACCESS_TOKEN\")\n",
        "    if r.status_code >= 400:\n",
        "        raise SystemExit(f\"Spotify API error {r.status_code}. Body: {r.text}\")\n",
        "\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def get_final_answer(user_query, spotify_json, search_type):\n",
        "    system = (\n",
        "        \"You are an Expert Data Analyst for music metadata. \"\n",
        "        \"Produce a short and clear answer for the user based only on the provided Spotify JSON. \"\n",
        "        \"Do not invent facts. \"\n",
        "        \"If a field is missing, say that the data is not available. \"\n",
        "        \"Keep claims grounded in the supplied JSON. \"\n",
        "        \"Return only the fields required by the schema.\"\n",
        "    )\n",
        "\n",
        "    user_payload = {\n",
        "        \"original_query\": user_query,\n",
        "        \"search_type\": search_type,\n",
        "        \"spotify_raw_json\": spotify_json,\n",
        "        \"instructions\": [\n",
        "            \"If search_type is artist, focus on id, followers, genres, popularity, images, name, external urls.\",\n",
        "            \"If search_type is album, focus on id, name, release date, artists, total tracks, external urls.\",\n",
        "            \"If several items are returned, highlight the top one by Spotify popularity or the first item. Mention others briefly.\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    resp = client.chat.completions.parse(\n",
        "        model = openai_model,\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(user_payload, ensure_ascii=False)},\n",
        "        ],\n",
        "        response_format = MusicAnswer,\n",
        "        temperature = 0.2,\n",
        "    )\n",
        "\n",
        "    answer_obj = resp.choices[0].message.parsed\n",
        "    if not isinstance(answer_obj, MusicAnswer):\n",
        "        raise RuntimeError(\"Could not parse final answer\")\n",
        "    return answer_obj\n",
        "\n",
        "\n",
        "def investigate_music_query(user_query):\n",
        "    api_args = get_api_args(user_query)\n",
        "    spotify_json = call_spotify_api(api_args)\n",
        "    final_answer = get_final_answer(user_query, spotify_json, api_args.search_type)\n",
        "    return final_answer\n",
        "\n",
        "\n",
        "user_query = input(\"Enter your music question: \").strip()\n",
        "if not user_query:\n",
        "    user_query = \"Tell me about the 'Is This It'\"\n",
        "\n",
        "result = investigate_music_query(user_query)\n",
        "print(result.model_dump_json(indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
