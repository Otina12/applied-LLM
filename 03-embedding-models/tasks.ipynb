{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VfL-Tgd6vVpX"
      },
      "outputs": [],
      "source": [
        "from gensim import downloader as gensim_api\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFmLpjyMwCUD"
      },
      "source": [
        "# ðŸ“œ Instructions\n",
        "\n",
        "Define two sentences with an ambiguous word, like \"bank\":\n",
        "```python\n",
        "sentence_a = \"He sat on the river bank.\"\n",
        "sentence_b = \"He went to the bank to deposit money.\"\n",
        "```\n",
        "\n",
        "## Word2Vec (Static):\n",
        "\n",
        "1. Load a pre-trained Word2Vec model (e.g., `glove-wiki-gigaword-100` from gensim.downloader).\n",
        "2. Get the vector for the word \"bank\".\n",
        "3. **Observation:** Notice that you can only get one vector for \"bank,\" regardless of context.\n",
        "\n",
        "## BERT (Contextual):\n",
        "\n",
        "1. Load a pre-trained BERT model and tokenizer (e.g., `bert-base-uncased`).\n",
        "2. Tokenize sentence_a and pass it to the model to get its hidden states (embeddings).\n",
        "3. Find the token index corresponding to the word \"bank\" in sentence_a and extract its vector from the last hidden state.\n",
        "4. Repeat the process for sentence_b.\n",
        "5. You now have two different vectors for \"bank\": `bank_vector_a` and `bank_vector_b`.\n",
        "6. Calculate the cosine similarity between `bank_vector_a` and `bank_vector_b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_a = \"He sat on the river bank.\"\n",
        "sentence_b = \"He went to the bank to deposit money.\"\n",
        "target_word = \"bank\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6T89Uil1wt4f"
      },
      "outputs": [],
      "source": [
        "# Word2Vec (Static)\n",
        "\n",
        "glove = gensim_api.load(\"glove-wiki-gigaword-100\") # load only once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence A tokens: ['he', 'sat', 'on', 'the', 'river', 'bank']\n",
            "Sentence B tokens: ['he', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money']\n",
            "\n",
            "Positions of 'bank' in A: 5\n",
            "Positions of 'bank' in B: 4\n",
            "\n",
            "First 5 values of 'bank' in sentence a: [ 0.41869  -0.92211   0.048684  0.11798   0.22062 ]\n",
            "First 5 values of 'bank' in sentence b: [ 0.41869  -0.92211   0.048684  0.11798   0.22062 ]\n",
            "\n",
            "'bank' vectors across the sentences are the same\n"
          ]
        }
      ],
      "source": [
        "def tokenize(text):\n",
        "    return [t.strip(\".,!?;:\").lower() for t in text.split()]\n",
        "\n",
        "tokens_a = tokenize(sentence_a)\n",
        "tokens_b = tokenize(sentence_b)\n",
        "\n",
        "idx_a = [i for i, t in enumerate(tokens_a) if t == target_word][0]\n",
        "idx_b = [i for i, t in enumerate(tokens_b) if t == target_word][0]\n",
        "\n",
        "print(f\"Sentence A tokens: {tokens_a}\")\n",
        "print(f\"Sentence B tokens: {tokens_b}\")\n",
        "print()\n",
        "print(f\"Positions of '{target_word}' in A: {idx_a}\")\n",
        "print(f\"Positions of '{target_word}' in B: {idx_b}\")\n",
        "print()\n",
        "\n",
        "bank_a = tokens_a[idx_a]\n",
        "bank_b = tokens_b[idx_b]\n",
        "\n",
        "bank_vec_in_a = glove.get_vector(bank_a)\n",
        "bank_vec_in_b = glove.get_vector(bank_b)\n",
        "\n",
        "print(f\"First 5 values of 'bank' in sentence a: {bank_vec_in_a[:5]}\")\n",
        "print(f\"First 5 values of 'bank' in sentence b: {bank_vec_in_b[:5]}\")\n",
        "print()\n",
        "\n",
        "same_values = np.allclose(bank_vec_in_a, bank_vec_in_b)\n",
        "print(f\"'bank' vectors across the sentences are {'' if same_values else 'not '}the same\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT (Contextual)\n",
        "\n",
        "# run only once\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens for 'He sat on the river bank.':\n",
            "['[CLS]', 'he', 'sat', 'on', 'the', 'river', 'bank', '.', '[SEP]']\n",
            "\n",
            "Tokens for 'He went to the bank to deposit money.':\n",
            "['[CLS]', 'he', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.', '[SEP]']\n",
            "\n",
            "Cosine similarity between 'bank' in two contexts: 0.472152\n"
          ]
        }
      ],
      "source": [
        "def get_token_vector(sentence, target):\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    outputs = model(**tokens)\n",
        "    last_hidden_state = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    token_strs = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "    print(f\"\\nTokens for '{sentence}':\")\n",
        "    print(token_strs)\n",
        "\n",
        "    target_indices = [i for i, tok in enumerate(token_strs) if target in tok]\n",
        "    if not target_indices:\n",
        "        raise ValueError(f\"Target '{target}' not found in tokenized output.\")\n",
        "\n",
        "    idx = target_indices[0]\n",
        "    vec = last_hidden_state[idx].detach().numpy()\n",
        "    return vec\n",
        "\n",
        "vec_a = get_token_vector(sentence_a, target_word)\n",
        "vec_b = get_token_vector(sentence_b, target_word)\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    return float(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)))\n",
        "\n",
        "similarity = cosine_similarity(vec_a, vec_b)\n",
        "\n",
        "print(f\"\\nCosine similarity between 'bank' in two contexts: {similarity:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1vd45Mwsia"
      },
      "source": [
        "## ðŸ“œ Instructions\n",
        "\n",
        "Define a query sentence and a list of candidate sentences:\n",
        "```python\n",
        "query = \"A man is playing a guitar.\"\n",
        "candidates = [\n",
        "    \"A person is strumming a musical instrument.\",  # High similarity, different words\n",
        "    \"A woman is playing a guitar.\",                 # High similarity, different subject\n",
        "    \"A man is eating a sandwich.\",                  # Low similarity, same subject\n",
        "    \"The guitar is being played by a man.\",         # High similarity, passive voice\n",
        "    \"Dogs are chasing a cat.\"                       # No similarity\n",
        "]\n",
        "```\n",
        "\n",
        "### Method 1: Word2Vec (with Averaging)\n",
        "\n",
        "For the query and each candidate sentence:\n",
        "1. Get the Word2Vec vector for each word (you may want to lowercase and remove stop words).\n",
        "2. Create a single \"sentence vector\" by averaging the vectors of all words in the sentence.\n",
        "3. Calculate the cosine similarity between the query vector and each candidate vector.\n",
        "4. Rank the candidates from most to least similar to the query.\n",
        "\n",
        "### Method 2: BERT (with Mean Pooling)\n",
        "\n",
        "For the query and each candidate sentence:\n",
        "1. Get the BERT token embeddings (last hidden state) for the sentence.\n",
        "2. Create a single \"sentence vector\" by averaging all the token embeddings (this is a common but often sub-optimal strategy called \"mean pooling\").\n",
        "3. Calculate the cosine similarity between the query vector and each candidate vector.\n",
        "4. Rank the candidates.\n",
        "\n",
        "### Method 3: Sentence-BERT (The Right Tool)\n",
        "\n",
        "1. Load a pre-trained Sentence-BERT model (e.g., `all-MiniLM-L6-v2`).\n",
        "2. Use the `model.encode()` function to get a single, high-quality sentence embedding for the query and each candidate.\n",
        "3. Calculate the cosine similarity between the query embedding and each candidate embedding.\n",
        "4. Rank the candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7WSPt8iryTF7"
      },
      "outputs": [],
      "source": [
        "query = \"A man is playing a guitar.\"\n",
        "candidates = [\n",
        "    \"A person is strumming a musical instrument.\",  # High similarity, different words\n",
        "    \"A woman is playing a guitar.\",                 # High similarity, different subject\n",
        "    \"A man is eating a sandwich.\",                  # Low similarity, same subject\n",
        "    \"The guitar is being played by a man.\",         # High similarity, passive voice\n",
        "    \"Dogs are chasing a cat.\"                       # No similarity\n",
        "]\n",
        "\n",
        "def cosine(u, v):\n",
        "    u = np.asarray(u, dtype = np.float32)\n",
        "    v = np.asarray(v, dtype = np.float32)\n",
        "    num = float(np.dot(u, v))\n",
        "    den = float(np.linalg.norm(u) * np.linalg.norm(v))\n",
        "    return 0.0 if den == 0.0 else num / den\n",
        "\n",
        "def rank_by_similarity(query_vec, cand_vecs, labels):\n",
        "    scores = [cosine(query_vec, v) for v in cand_vecs]\n",
        "    order = np.argsort(scores)[::-1]\n",
        "    return [(labels[i], scores[i]) for i in order]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. 0.9838  |  The guitar is being played by a man.\n",
            "2. 0.9730  |  A woman is playing a guitar.\n",
            "3. 0.7859  |  A person is strumming a musical instrument.\n",
            "4. 0.5927  |  A man is eating a sandwich.\n",
            "5. 0.4672  |  Dogs are chasing a cat.\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec (with Averaging)\n",
        "\n",
        "stopwords = {\n",
        "    \"a\",\"an\",\"the\",\"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"to\",\"of\",\"and\",\"or\",\"for\",\"in\",\"on\",\"at\",\"by\",\"with\",\"as\",\"that\",\"this\",\n",
        "    \"it\",\"its\",\"his\",\"her\",\"their\",\"he\",\"she\",\"they\",\"you\",\"i\"\n",
        "}\n",
        "\n",
        "def tokenize(text):\n",
        "    return [t.strip(\".,!?;:()\\\"'\").lower() for t in text.split() if t.strip(\".,!?;:()\\\"'\")]\n",
        "\n",
        "def sentence_vec(text):\n",
        "    tokens = [t for t in tokenize(text) if t not in stopwords and t in glove.key_to_index]\n",
        "    token_vectors = [glove.get_vector(t) for t in tokens]\n",
        "    matrix = np.stack(token_vectors, axis = 0)\n",
        "    return matrix.mean(axis = 0)\n",
        "\n",
        "query_vector = sentence_vec(query)\n",
        "candidate_vectors = [sentence_vec(s) for s in candidates]\n",
        "ranked = rank_by_similarity(query_vector, candidate_vectors, candidates)\n",
        "\n",
        "for i, (sent, score) in enumerate(ranked, 1):\n",
        "    print(f\"{i}. {score:.4f}  |  {sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT (Mean Pooling) similarity results:\n",
            "1. 0.9841  |  A woman is playing a guitar.\n",
            "2. 0.8898  |  The guitar is being played by a man.\n",
            "3. 0.8878  |  A man is eating a sandwich.\n",
            "4. 0.7919  |  Dogs are chasing a cat.\n",
            "5. 0.7911  |  A person is strumming a musical instrument.\n"
          ]
        }
      ],
      "source": [
        "# BERT (with Mean Pooling)\n",
        "\n",
        "def encode_mean(sentence):\n",
        "    inputs = tokenizer(sentence, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    last_hidden = outputs.last_hidden_state\n",
        "    mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "\n",
        "    summed = (last_hidden * mask).sum(dim=1)\n",
        "    counts = mask.sum(dim=1).clamp(min=1)\n",
        "    pooled = summed / counts\n",
        "\n",
        "    return pooled.squeeze(0).cpu().numpy()\n",
        "\n",
        "query_vector = encode_mean(query)\n",
        "candidate_vectors = np.vstack([encode_mean(s) for s in candidates])\n",
        "\n",
        "ranked = rank_by_similarity(query_vector, candidate_vectors, candidates)\n",
        "\n",
        "print(\"\\nBERT (Mean Pooling) similarity results:\")\n",
        "for i, (sent, score) in enumerate(ranked, 1):\n",
        "    print(f\"{i}. {score:.4f}  |  {sent}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
