{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqP8pdvURfQt"
      },
      "source": [
        "# üß™ Lab Task: Benchmarking Vector Indexing and Retrieval Methods\n",
        "\n",
        "## üéØ Objective\n",
        "In this lab, you will **experiment with different vector indexing methods** and **measure their performance** for document retrieval using embedding-based search.\n",
        "\n",
        "You will:\n",
        "1. Generate text embeddings using a **Sentence Transformer** model.\n",
        "2. Index the embeddings using **two approximate nearest neighbor (ANN)** methods.\n",
        "3. Measure and compare **indexing time**, **retrieval latency**, and **retrieval quality**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Background\n",
        "\n",
        "In Retrieval-Augmented Generation (RAG) systems, a **vector database** is used to efficiently find semantically similar documents to a query.  \n",
        "Different indexing methods (e.g., **Flat**, **HNSW**, **IVF**) trade off between **speed**, **accuracy**, and **memory usage**.\n",
        "\n",
        "Understanding these trade-offs helps you design RAG systems that are **efficient** and **scalable**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Task Description\n",
        "\n",
        "### Step 1. Prepare the data\n",
        "- Create a small **corpus of text documents** (e.g., 1000‚Äì5000 Wikipedia sentences, news snippets, or sample text paragraphs).\n",
        "- Create a separate list of **query sentences** for evaluation (e.g., 10‚Äì20 queries).\n",
        "\n",
        "### Step 2. Generate embeddings\n",
        "- Use a **Sentence Transformer** model (e.g., `\"all-MiniLM-L6-v2\"` or any other from [https://huggingface.co/sentence-transformers/models](https://huggingface.co/sentence-transformers/models)).\n",
        "- Encode all documents and queries into embedding vectors.\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "doc_embeddings = model.encode(docs, normalize_embeddings=True)\n",
        "query_embeddings = model.encode(queries, normalize_embeddings=True)\n",
        "```\n",
        "\n",
        "# Step 3. Build Indexes\n",
        "\n",
        "You will build two different FAISS indexes and benchmark them:\n",
        "\n",
        "## üîπ Index 1: Flat (Exact Search)\n",
        "\n",
        "* Use `faiss.IndexFlatL2` (exact nearest neighbor search).\n",
        "* **Pros:** Perfect recall (most accurate).\n",
        "* **Cons:** Slow for large datasets.\n",
        "\n",
        "## üîπ Index 2: HNSW (Approximate Search)\n",
        "\n",
        "* Use `faiss.IndexHNSWFlat`.\n",
        "* Set appropriate parameters (e.g., `M=32`, `efConstruction=100`).\n",
        "* **Pros:** Much faster search; small loss in recall.\n",
        "\n",
        "### Example setup:\n",
        "```python\n",
        "import faiss\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "dim = doc_embeddings.shape[1]\n",
        "\n",
        "# Flat index\n",
        "index_flat = faiss.IndexFlatL2(dim)\n",
        "\n",
        "# HNSW index\n",
        "index_hnsw = faiss.IndexHNSWFlat(dim, 32)\n",
        "index_hnsw.hnsw.efConstruction = 100\n",
        "index_hnsw.hnsw.efSearch = 50\n",
        "```\n",
        "\n",
        "# Step 4. Benchmarking\n",
        "\n",
        "Measure and compare the following metrics:\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| **Indexing Time** | Time to add all embeddings to the index. |\n",
        "| **Retrieval Latency** | Average time to find top-k results for each query. |\n",
        "| **Recall@k** | How often the retrieved top-k includes the true nearest neighbor. |\n",
        "| **Memory Usage** (optional) | Approximate RAM usage of each index type. |\n",
        "\n",
        "You can record times using `time.time()` or `time.perf_counter()`.\n",
        "\n",
        "\n",
        "```markdown\n",
        "## üí° Tips\n",
        "\n",
        "* Normalize embeddings before indexing for stable distance comparisons.\n",
        "* Use `faiss.normalize_L2()` if needed.\n",
        "* Experiment with `efSearch` and `M` parameters in HNSW to see how they impact performance.\n",
        "``\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynFqP-g1rLTx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_path = \"assets/text.txt\"\n",
        "queries_path = \"assets/queries.txt\"\n",
        "text = []\n",
        "queries = []\n",
        "\n",
        "with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        s = line.strip()\n",
        "        if s:\n",
        "            text.append(s)\n",
        "\n",
        "with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        s = line.strip()\n",
        "        if s:\n",
        "            queries.append(s)\n",
        "\n",
        "if len(text) == 0 or len(queries) == 0:\n",
        "    raise ValueError(\"Either text or queries are not present\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_embeddings = model.encode(text, normalize_embeddings=True)\n",
        "\n",
        "dim = text_embeddings.shape[1]\n",
        "ntotal = text_embeddings.shape[0]\n",
        "\n",
        "num_queries = 10\n",
        "rng = np.random.default_rng(0)\n",
        "query_idx = rng.choice(ntotal, size=num_queries, replace=False)\n",
        "query_embeddings = text_embeddings[query_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss.normalize_L2(text_embeddings)\n",
        "faiss.normalize_L2(query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flat index built in 0.002s\n"
          ]
        }
      ],
      "source": [
        "# Flat index\n",
        "\n",
        "index_flat = faiss.IndexFlatL2(dim)\n",
        "t0 = time.perf_counter()\n",
        "index_flat.add(text_embeddings)\n",
        "t_flat = time.perf_counter() - t0\n",
        "\n",
        "print(f\"Flat index built in {t_flat:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HNSW index built in 0.172s\n"
          ]
        }
      ],
      "source": [
        "# HNSW index\n",
        "\n",
        "index_hnsw = faiss.IndexHNSWFlat(dim, 32)\n",
        "index_hnsw.hnsw.efConstruction = 100\n",
        "t0 = time.perf_counter()\n",
        "index_hnsw.add(text_embeddings)\n",
        "t_hnsw = time.perf_counter() - t0\n",
        "\n",
        "print(f\"HNSW index built in {t_hnsw:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flat index search time per query: 0.82 ms\n"
          ]
        }
      ],
      "source": [
        "# Flat search\n",
        "_ = index_flat.search(query_embeddings[:min(8, len(query_embeddings))], 1)\n",
        "t0 = time.perf_counter()\n",
        "D_true, I_true = index_flat.search(query_embeddings, 1)\n",
        "avg_ms_flat = (time.perf_counter() - t0) / num_queries * 1000.0\n",
        "true_nn = I_true[:, 0]\n",
        "\n",
        "print(f\"Flat index search time per query: {avg_ms_flat:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_hnsw_for_k(M, ef_construction, ef_search, k):\n",
        "    index_hnsw = faiss.IndexHNSWFlat(dim, M)\n",
        "    index_hnsw.hnsw.efConstruction = ef_construction\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    index_hnsw.add(text_embeddings)\n",
        "    build_s = time.perf_counter() - t0\n",
        "\n",
        "    index_hnsw.hnsw.efSearch = ef_search\n",
        "    _ = index_hnsw.search(query_embeddings[:min(8, len(query_embeddings))], k)\n",
        "    t0 = time.perf_counter()\n",
        "    D_h, I_h = index_hnsw.search(query_embeddings, k)\n",
        "    latency_ms = (time.perf_counter() - t0) / num_queries * 1000.0\n",
        "\n",
        "    hits = 0\n",
        "    for i in range(len(true_nn)):\n",
        "        if true_nn[i] in I_h[i, :k]:\n",
        "            hits += 1\n",
        "    recall_atk = hits / len(true_nn)\n",
        "\n",
        "    mem_flat_bytes = ntotal * dim * 4\n",
        "    mem_links_bytes = ntotal * M * 8\n",
        "    approx_mem = mem_flat_bytes + mem_links_bytes\n",
        "\n",
        "    return build_s, latency_ms, recall_atk, approx_mem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== HNSW sweep results for k = 1 ===\n",
            "\n",
            "M   efSearch  build_s  latency_ms  recall@k  approx_RAM\n",
            " 4        4   0.135     0.00        0.500      6.64 MB\n",
            " 4        8   0.152     0.01        0.500      6.64 MB\n",
            " 4       32   0.145     0.01        0.600      6.64 MB\n",
            " 4      100   0.157     0.04        0.800      6.64 MB\n",
            " 8        4   0.182     0.01        0.600      6.77 MB\n",
            " 8        8   0.189     0.01        0.600      6.77 MB\n",
            " 8       32   0.216     0.01        0.500      6.77 MB\n",
            " 8      100   0.186     0.03        0.500      6.77 MB\n",
            "16        4   0.176     0.01        0.600      7.05 MB\n",
            "16        8   0.185     0.01        0.700      7.05 MB\n",
            "16       32   0.173     0.02        0.800      7.05 MB\n",
            "16      100   0.194     0.03        0.500      7.05 MB\n",
            "32        4   0.177     0.01        0.400      7.59 MB\n",
            "32        8   0.178     0.01        0.500      7.59 MB\n",
            "32       32   0.177     0.02        0.300      7.59 MB\n",
            "32      100   0.174     0.04        0.800      7.59 MB\n",
            "\n",
            "Suggested setting for this k\n",
            "M=16, efSearch=32, recall@1=0.800, latency=0.02 ms, build_s=0.17, RAM=7.05 MB\n",
            "\n",
            "=== HNSW sweep results for k = 2 ===\n",
            "\n",
            "M   efSearch  build_s  latency_ms  recall@k  approx_RAM\n",
            " 4        4   0.160     0.01        0.800      6.64 MB\n",
            " 4        8   0.152     0.01        1.000      6.64 MB\n",
            " 4       32   0.163     0.01        0.900      6.64 MB\n",
            " 4      100   0.152     0.02        1.000      6.64 MB\n",
            " 8        4   0.191     0.01        1.000      6.77 MB\n",
            " 8        8   0.188     0.01        1.000      6.77 MB\n",
            " 8       32   0.184     0.01        1.000      6.77 MB\n",
            " 8      100   0.194     0.03        1.000      6.77 MB\n",
            "16        4   0.176     0.01        1.000      7.05 MB\n",
            "16        8   0.175     0.01        1.000      7.05 MB\n",
            "16       32   0.177     0.02        1.000      7.05 MB\n",
            "16      100   0.175     0.03        1.000      7.05 MB\n",
            "32        4   0.178     0.01        1.000      7.59 MB\n",
            "32        8   0.180     0.01        1.000      7.59 MB\n",
            "32       32   0.182     0.02        1.000      7.59 MB\n",
            "32      100   0.181     0.05        1.000      7.59 MB\n",
            "\n",
            "Suggested setting for this k\n",
            "M=8, efSearch=4, recall@2=1.000, latency=0.01 ms, build_s=0.19, RAM=6.77 MB\n",
            "\n",
            "=== HNSW sweep results for k = 5 ===\n",
            "\n",
            "M   efSearch  build_s  latency_ms  recall@k  approx_RAM\n",
            " 4        4   0.160     0.01        0.700      6.64 MB\n",
            " 4        8   0.157     0.01        0.900      6.64 MB\n",
            " 4       32   0.163     0.02        1.000      6.64 MB\n",
            " 4      100   0.158     0.02        1.000      6.64 MB\n",
            " 8        4   0.196     0.01        0.900      6.77 MB\n",
            " 8        8   0.193     0.01        1.000      6.77 MB\n",
            " 8       32   0.193     0.02        1.000      6.77 MB\n",
            " 8      100   0.199     0.03        1.000      6.77 MB\n",
            "16        4   0.187     0.01        1.000      7.05 MB\n",
            "16        8   0.181     0.01        1.000      7.05 MB\n",
            "16       32   0.187     0.02        1.000      7.05 MB\n",
            "16      100   0.175     0.04        1.000      7.05 MB\n",
            "32        4   0.187     0.01        1.000      7.59 MB\n",
            "32        8   0.186     0.01        1.000      7.59 MB\n",
            "32       32   0.190     0.02        1.000      7.59 MB\n",
            "32      100   0.188     0.04        1.000      7.59 MB\n",
            "\n",
            "Suggested setting for this k\n",
            "M=8, efSearch=8, recall@5=1.000, latency=0.01 ms, build_s=0.19, RAM=6.77 MB\n"
          ]
        }
      ],
      "source": [
        "top_k_values = [1, 2, 5]\n",
        "M_values = [4, 8, 16, 32]\n",
        "ef_construction = 100\n",
        "ef_search_values = [4, 8, 32, 100]\n",
        "\n",
        "def human_bytes(n):\n",
        "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
        "    if n <= 0:\n",
        "        return \"0 B\"\n",
        "    i = int(math.floor(math.log(n, 1024)))\n",
        "    p = 1024 ** i\n",
        "    return f\"{n / p:.2f} {units[i]}\"\n",
        "\n",
        "for k in top_k_values:\n",
        "    print(f\"\\n=== HNSW sweep results for k = {k} ===\\n\")\n",
        "    print(\"M   efSearch  build_s  latency_ms  recall@k  approx_RAM\")\n",
        "\n",
        "    results_k = []\n",
        "\n",
        "    for M in M_values:\n",
        "        for ef in ef_search_values:\n",
        "            build_s, latency_ms, recall_atk, ram = test_hnsw_for_k(M, ef_construction, ef, k)\n",
        "            results_k.append((M, ef, build_s, latency_ms, recall_atk, ram))\n",
        "            print(\n",
        "                f\"{M:>2} {ef:>8}   \"\n",
        "                f\"{build_s:.3f}     \"\n",
        "                f\"{latency_ms:.2f}        \"\n",
        "                f\"{recall_atk:.3f}      \"\n",
        "                f\"{human_bytes(ram)}\"\n",
        "            )\n",
        "\n",
        "    best = sorted(results_k, key=lambda r: (-r[4], r[3]))[0]\n",
        "    print(\"\\nSuggested setting for this k\")\n",
        "    print(\n",
        "        f\"M={best[0]}, efSearch={best[1]}, \"\n",
        "        f\"recall@{k}={best[4]:.3f}, latency={best[3]:.2f} ms, \"\n",
        "        f\"build_s={best[2]:.2f}, RAM={human_bytes(best[5])}\"\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
