{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmXShYJPxXKC"
      },
      "source": [
        "# ðŸ§ª Lab Task: The \"Smart Search\" Agent\n",
        "\n",
        "Your task is to build a conversational agent that answers user questions using a single, powerful `web_search` tool.\n",
        "\n",
        "This tool is \"smart\": your Python implementation of the function must internally and invisibly call a (mock or real) small LLM to rewrite the query before it ever calls the final search API. The main LLM will be unaware this \"rewrite\" step is happening.\n",
        "\n",
        "## Standard Task\n",
        "\n",
        "### Define the Single Function Tool:\n",
        "\n",
        "From the LLM's perspective, it only knows about one tool: `web_search(user_query: str) -> List[SearchResult]`\n",
        "\n",
        "The `user_query` will be the user's raw, conversational text (e.g., \"Why is the sky blue?\").\n",
        "\n",
        "### Implement the \"Smart\" Function:\n",
        "\n",
        "Your Python code that defines `web_search` must execute this internal sequence:\n",
        "\n",
        "**Step 1 (Internal):** Take the `user_query` string.\n",
        "\n",
        "**Step 2 (Internal):** Pass this string to a smaller LLM API (or just a hard-coded prompt) with instructions like, \"You are a search query expert. Convert this question into 3-5 optimal keywords.\"\n",
        "\n",
        "**Step 3 (Internal):** Receive the `rewritten_query` (e.g., \"Rayleigh scattering atmosphere\") from that internal call.\n",
        "\n",
        "**Step 4 (Internal):** Use this `rewritten_query` to call the actual search API (e.g., Google, Bing, or a mock JSON file).\n",
        "\n",
        "**Step 5 (Return):** Return the final list of `SearchResult` objects to the main LLM.\n",
        "\n",
        "### Implement the Agent Logic:\n",
        "\n",
        "- The LLM's main prompt should instruct it to use the `web_search` tool whenever it needs external information.\n",
        "- The LLM will call the tool with the user's messy query.\n",
        "- It will receive the clean search results back (unaware of the internal rewrite).\n",
        "- It must then use these results to synthesize a final, helpful answer.\n",
        "\n",
        "### Example Flow:\n",
        "\n",
        "**User:** \"What's the best way to get from Warsaw to Berlin?\"\n",
        "\n",
        "**LLM (call 1):** `web_search(user_query=\"What's the best way to get from Warsaw to Berlin?\")`\n",
        "\n",
        "**Your Code (Internal Step 1-3):** Calls rewriter â†’ gets back \"Warsaw to Berlin travel options train bus flight\"\n",
        "\n",
        "**Your Code (Internal Step 4):** Calls search API with the new query â†’ gets back `[{title: \"Trains Warsaw-Berlin...\", snippet: \"...\", url: \"...\"}, ...]`\n",
        "\n",
        "**Your Code (returns to LLM):** `[{title: \"Trains Warsaw-Berlin...\", snippet: \"...\", url: \"...\"}, ...]`\n",
        "\n",
        "**LLM (final answer):** \"You can travel from Warsaw to Berlin by train, bus, or plane. According to 'Trains Warsaw-Berlin...', the train takes about 6 hours...\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ðŸš€ Advanced Task\n",
        "\n",
        "### Add a Second Tool for Summarization:\n",
        "\n",
        "Now, add a new, separate tool that the LLM is aware of: `scrape_and_summarize(urls: List[str]) -> str`\n",
        "\n",
        "This function (which you'll implement) takes a list of URLs and returns a block of (mocked or real) text summarizing their content.\n",
        "\n",
        "### Modify the Agent Logic:\n",
        "\n",
        "- Update the LLM's main prompt. Instruct it to first call `web_search` as before.\n",
        "- Then, it must analyze the search snippets.\n",
        "- If the snippets are too short or don't fully answer the question, it should make a second, follow-up call to `scrape_and_summarize` using the top 2-3 URLs from the first call.\n",
        "- This tests the LLM's ability to chain tools and make decisions based on intermediate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jByOdSDXxdIU"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from openai import OpenAI\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MXpi6rNQDjNN"
      },
      "outputs": [],
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key = openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hde38KmD-dwC"
      },
      "outputs": [],
      "source": [
        "def small_llm_rewrite(user_query):\n",
        "    system_prompt = (\n",
        "        \"You are a search query expert. \"\n",
        "        \"Read the user question. \"\n",
        "        \"Return only 3 to 6 short keywords. \"\n",
        "        \"Do not write sentences. \"\n",
        "        \"Do not add explanations.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        \"User question:\\n\"\n",
        "        f\"{user_query}\\n\\n\"\n",
        "        \"Return 3 to 6 keywords separated by spaces.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-4.1-mini\",\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_completion_tokens = 30\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].message.content.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nzm1O8jV-z4z"
      },
      "outputs": [],
      "source": [
        "you_api_key = os.getenv(\"YOU_SEARCH_API_KEY\")\n",
        "you_search_url = \"https://api.ydc-index.io/search\"\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "\n",
        "def call_you_search_api(query):\n",
        "    headers = {\n",
        "        \"X-API-Key\": you_api_key\n",
        "    }\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"count\": 20\n",
        "    }\n",
        "\n",
        "    resp = requests.get(you_search_url, headers = headers, params = params, timeout = 10)\n",
        "    data = resp.json()\n",
        "    hits = data.get(\"hits\", [])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for item in hits:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        url = item.get(\"url\", \"\")\n",
        "\n",
        "        desc = item.get(\"description\", \"\")\n",
        "        snippets = item.get(\"snippets\", [])\n",
        "        snippet_text = \" \".join(snippets) if snippets else desc\n",
        "\n",
        "        results.append(SearchResult(\n",
        "            title = title,\n",
        "            snippet = snippet_text,\n",
        "            url = url\n",
        "        ))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rybZgx_gAXbx"
      },
      "outputs": [],
      "source": [
        "def web_search(user_query):\n",
        "    rewritten_query = small_llm_rewrite(user_query)\n",
        "    print(f\"rewrote query to: {rewritten_query}\")\n",
        "    results = call_you_search_api(rewritten_query)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69h-bbNwByIf"
      },
      "outputs": [],
      "source": [
        "def scrape_page_text(url):\n",
        "    try:\n",
        "        resp = requests.get(\n",
        "            url,\n",
        "            timeout = 8,\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        )\n",
        "\n",
        "        if resp.status_code != 200:\n",
        "            print(\"could not fetch:\", url, resp.status_code)\n",
        "            return \"\"\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"form\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator = \" \")\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        if not text:\n",
        "            print(\"empty content:\", url)\n",
        "            return \"\"\n",
        "\n",
        "        print(\"fetched\", url)\n",
        "        return text[:4000]\n",
        "    except Exception as e:\n",
        "        print(\"error scraping:\", url, e)\n",
        "        return \"\"\n",
        "\n",
        "def scrape_and_summarize(urls):\n",
        "    texts = []\n",
        "    for url in urls:\n",
        "        t = scrape_page_text(url)\n",
        "        if t:\n",
        "            texts.append(t)\n",
        "\n",
        "    if not texts:\n",
        "        return \"Could not read the pages.\"\n",
        "\n",
        "    joined = \"\\n\\n\".join(texts)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-4.1-mini\",\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Read the content and write a short clear summary.\"},\n",
        "            {\"role\": \"user\", \"content\": joined}\n",
        "        ],\n",
        "        temperature = 0.3,\n",
        "        max_tokens = 256\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DK8Wj066BYyb"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web_search\",\n",
        "            \"description\": \"Search the web for information relevant to the user query.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Original user question.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"user_query\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"scrape_and_summarize\",\n",
        "            \"description\": \"Fetch the given URLs and return a short summary.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"urls\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"string\"},\n",
        "                        \"description\": \"List of URLs from search results.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"urls\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K6fSGp0bBnlI"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are SmartSearchAgent.\n",
        "\n",
        "You can use two tools:\n",
        "1. web_search(user_query: str)\n",
        "2. scrape_and_summarize(urls: List[str])\n",
        "\n",
        "When you need external information:\n",
        "First call web_search with the user query.\n",
        "Look at the SearchResult objects.\n",
        "If snippets are enough, answer based on them.\n",
        "If snippets are not enough, choose 2 or 3 URLs.\n",
        "Call scrape_and_summarize with these URLs.\n",
        "Use the summary and the snippets to answer.\n",
        "Use simple and clear language.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PNsOZFpDBaFP"
      },
      "outputs": [],
      "source": [
        "def call_oai(user_prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    first = client.chat.completions.create(\n",
        "        model = \"gpt-4.1\",\n",
        "        messages = messages,\n",
        "        tools = tools,\n",
        "        tool_choice = \"auto\" # {\"type\": \"function\", \"function\": {\"name\": \"web_search\"}}\n",
        "    )\n",
        "\n",
        "    msg = first.choices[0].message\n",
        "\n",
        "    if msg.tool_calls:\n",
        "        for tool_call in msg.tool_calls:\n",
        "            name = tool_call.function.name\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            if name == \"web_search\":\n",
        "                print(\"using web search tool...\")\n",
        "                search_results = web_search(args[\"user_query\"])\n",
        "                payload = [res.__dict__ for res in search_results]\n",
        "\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"tool_calls\": msg.tool_calls\n",
        "                })\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"name\": \"web_search\",\n",
        "                    \"content\": json.dumps(payload),\n",
        "                })\n",
        "\n",
        "                second = client.chat.completions.create(\n",
        "                    model = \"gpt-4.1\",\n",
        "                    messages = messages,\n",
        "                    tools = tools,\n",
        "                    tool_choice = {\"type\": \"function\", \"function\": {\"name\": \"scrape_and_summarize\"}} # \"auto\"\n",
        "                )\n",
        "\n",
        "                msg2 = second.choices[0].message\n",
        "\n",
        "                if msg2.tool_calls:\n",
        "                    for tc in msg2.tool_calls:\n",
        "                        if tc.function.name == \"scrape_and_summarize\":\n",
        "                            print(\"\\nusing scrape and summarize tool...\")\n",
        "                            args2 = json.loads(tc.function.arguments)\n",
        "                            summary = scrape_and_summarize(args2[\"urls\"])\n",
        "\n",
        "                            messages.append({\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"tool_calls\": msg2.tool_calls\n",
        "                            })\n",
        "                            messages.append({\n",
        "                                \"role\": \"tool\",\n",
        "                                \"tool_call_id\": tc.id,\n",
        "                                \"name\": \"scrape_and_summarize\",\n",
        "                                \"content\": summary\n",
        "                            })\n",
        "\n",
        "                            final = client.chat.completions.create(\n",
        "                                model = \"gpt-4.1\",\n",
        "                                messages = messages,\n",
        "                            )\n",
        "\n",
        "                            return final.choices[0].message.content\n",
        "\n",
        "                return msg2.content\n",
        "\n",
        "    return msg.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a0ZvYm_Ankk",
        "outputId": "e0be924a-920d-49ed-a14d-9a2c47559c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using web search tool...\n",
            "rewrote query to: stock market November 2025 decline causes factors\n",
            "\n",
            "using scrape and summarize tool...\n",
            "fetched https://en.wikipedia.org/wiki/2025_stock_market_crash\n",
            "fetched https://stockstotrade.com/stock-market-crash/\n",
            "fetched https://www.cnbc.com/2025/11/03/stock-market-today-live-updates.html\n",
            "\n",
            "==================================================\n",
            "\n",
            "The main reason the stock market was down in November 2025 is because of major policy changes, especially new tariffs introduced by U.S. President Donald Trump in April 2025. These tariffs affected nearly all sectors of the U.S. economy and led to intensified trade wars with China, Canada, and Mexico. The sudden and sweeping nature of these tariffs caused panic selling and sharp declines in global stock markets, especially in technology and AI-focused stocks, which were seen as overpriced and vulnerable.\n",
            "\n",
            "Other contributing factors included:\n",
            "- Concerns about overpriced tech stocks (like Nvidia, Apple, and Tesla) that had been leading the market.\n",
            "- Ongoing volatility and uncertainty from aggressive U.S. trade actions.\n",
            "- Decreased consumer confidence and spending, as seen in surveys.\n",
            "- Risk of a possible recession due to higher business costs and disrupted supply chains.\n",
            "\n",
            "By mid-2025, markets had partly recovered after some tariff pauses, but there was still a lot of caution. The market remained narrowly focused on a few large tech companies, making it sensitive to any new shocks. Overall, the drop in November 2025 was driven by a mix of trade policy uncertainty, high market valuations, and worries about economic growth.\n"
          ]
        }
      ],
      "source": [
        "query = \"why is the stock market down in November 2025?\"\n",
        "result = call_oai(query) # should use web_search tool\n",
        "print('\\n==================================================\\n')\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
