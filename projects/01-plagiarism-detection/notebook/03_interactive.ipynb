{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b6484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247dcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc41ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "\n",
    "base_dir = Path(\".\")\n",
    "data_dir = base_dir / \"data\"\n",
    "indexes_dir = base_dir / \"indexes\"\n",
    "\n",
    "dense_index_path = indexes_dir / \"dense_index.faiss\"\n",
    "dense_meta_path = indexes_dir / \"dense_meta.json\"\n",
    "bm25_path = indexes_dir / \"bm25_index.pkl\"\n",
    "test_dataset_path = data_dir / \"test_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c302997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dense index and meta\n",
    "\n",
    "if not dense_index_path.exists():\n",
    "    raise FileNotFoundError(f\"FAISS index not found at {dense_index_path}\")\n",
    "\n",
    "dense_index = faiss.read_index(str(dense_index_path))\n",
    "\n",
    "with open(dense_meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dense_meta = json.load(f)\n",
    "\n",
    "chunk_ids = dense_meta[\"chunk_ids\"]\n",
    "chunk_repos = dense_meta[\"repos\"]\n",
    "chunk_paths = dense_meta[\"paths\"]\n",
    "chunk_texts = dense_meta[\"texts\"]\n",
    "\n",
    "#print(f\"loaded dense index with {len(chunk_ids)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea30e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bm25\n",
    "\n",
    "with open(bm25_path, \"rb\") as f:\n",
    "    bm25_pack = pickle.load(f)\n",
    "\n",
    "bm25 = bm25_pack[\"bm25\"]\n",
    "bm25_chunks = bm25_pack[\"chunks\"]\n",
    "\n",
    "#print(f\"loaded bm25 index with {len(bm25_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e0e823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "emb_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "oai_client = None\n",
    "\n",
    "if openai_api_key:\n",
    "    oai_client = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2610823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def embed_code(text):\n",
    "    vec = emb_model.encode([text], convert_to_numpy=True)\n",
    "    return vec.astype(\"float32\")\n",
    "\n",
    "def tokenize_code(text):\n",
    "    return re.findall(r\"[A-Za-z_][A-Za-z0-9_]*\", text)\n",
    "\n",
    "def find_text_by_path(path):\n",
    "    for c in bm25_chunks:\n",
    "        if c[\"source_path\"] == path:\n",
    "            return c[\"text\"]\n",
    "    return None\n",
    "\n",
    "class PlagiarismResult(BaseModel): # Pydantic model for structured output from OpenAI\n",
    "    is_plagiarized: bool\n",
    "    reason: str\n",
    "    evidence: List[str] = []\n",
    "\n",
    "class DetectionResult: # each method will return this result\n",
    "    def __init__(self, method, is_plagiarized, reason, evidence_mine = \"\", evidence_oai = None):\n",
    "        self.method = method\n",
    "        self.is_plagiarized = is_plagiarized\n",
    "        self.reason = reason\n",
    "        self.evidence_mine = evidence_mine\n",
    "        self.evidence_oai = evidence_oai\n",
    "\n",
    "def llm_call(prompt) -> PlagiarismResult:\n",
    "    if oai_client is None:\n",
    "        return PlagiarismResult(is_plagiarized = False, reason = \"no OPENAI_API_KEY in env, skipping LLM detection\", evidence = [])\n",
    "    \n",
    "    oai_response = oai_client.chat.completions.parse(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful code plagiarism checker.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format = PlagiarismResult,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "\n",
    "    return oai_response.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2043d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure embedding search\n",
    "\n",
    "def detect_embedding(code_query, top_k = 10):\n",
    "    query_vec = embed_code(code_query)\n",
    "    distances, indexes = dense_index.search(query_vec, top_k)\n",
    "    evidence = []\n",
    "\n",
    "    # distance is L2 norm, and it's directly related to cosine similarity: D(a, b) = 2 - 2 * cos(Î˜)\n",
    "    # the less the distance, the more similar two documents are.\n",
    "    # we need to convert this into a formula to make similarity % more intuitive.\n",
    "    # simplest option is: similarity = 1 / (1 + distance)\n",
    "    # this way:\n",
    "    # when distance is 0, documents are the same, and similarity score will be 1.\n",
    "    # when distance is large, documents are very different, and similarity score will be significantly less than 1.\n",
    "    \n",
    "    for dist, idx in zip(distances[0], indexes[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        similarity = 1.0 / (1.0 + float(dist))\n",
    "        \n",
    "        if similarity >= similarity_threshold:\n",
    "            evidence.append(\n",
    "                {\n",
    "                    \"chunk_id\": chunk_ids[idx],\n",
    "                    \"path\": chunk_paths[idx],\n",
    "                    \"text\": chunk_texts[idx],\n",
    "                    \"similarity\": similarity\n",
    "                }\n",
    "            )\n",
    "\n",
    "    is_plagiarized = len(evidence) > 0\n",
    "\n",
    "    return DetectionResult(\n",
    "        method = \"pure_embedding\",\n",
    "        is_plagiarized = is_plagiarized,\n",
    "        reason = \"\",\n",
    "        evidence_mine = evidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a65e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct LLM analysis\n",
    "\n",
    "def build_prompt(corpus_snippets, code_query):\n",
    "    code_snippets = \"\\n\".join(\n",
    "        f\"[{i}]\\n```go\\n{snippet}\\n```\" for i, snippet in enumerate(corpus_snippets, 1)\n",
    "    )\n",
    "    return (\n",
    "        \"You are checking code plagiarism.\\n\"\n",
    "        \"You will get a query function and several candidate snippets.\\n\"\n",
    "        \"Answer with the provided response format.\\n\\n\"\n",
    "        f\"Query code:\\n```go\\n{code_query}\\n```\\n\\n\"\n",
    "        f\"Candidate snippets:\\n{code_snippets}\\n\\n\"\n",
    "        \"Think about structure, call order, and control flow, not only names.\"\n",
    "    )\n",
    "\n",
    "def detect_llm(code_query, top_n = 25):\n",
    "    corpus_snippets = [text for text in chunk_texts[:top_n]]\n",
    "    prompt = build_prompt(corpus_snippets, code_query)\n",
    "    result = llm_call(prompt)\n",
    "\n",
    "    return DetectionResult(\n",
    "        method = \"direct_llm\",\n",
    "        is_plagiarized = result.is_plagiarized,\n",
    "        reason = result.reason,\n",
    "        evidence_oai = result.evidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard RAG\n",
    "\n",
    "def detect_rag(code_query, top_k = 5):\n",
    "    query_vec = embed_code(code_query)\n",
    "    distances, indexes = dense_index.search(query_vec, top_k)\n",
    "\n",
    "    corpus_snippets = []\n",
    "    evidence = []\n",
    "\n",
    "    for dist, idx in zip(distances[0], indexes[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        path = chunk_paths[idx]\n",
    "        text = chunk_texts[idx]\n",
    "        \n",
    "        corpus_snippets.append(text)\n",
    "        \n",
    "        similarity = 1.0 / (1.0 + float(dist))\n",
    "        \n",
    "        if similarity > similarity_threshold:\n",
    "            evidence.append({\n",
    "                \"chunk_id\": chunk_ids[idx],\n",
    "                \"path\": path,\n",
    "                \"text\": text,\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "\n",
    "    prompt = build_prompt(corpus_snippets, code_query)\n",
    "    result = llm_call(prompt)\n",
    "\n",
    "    return DetectionResult(\n",
    "        method = \"rag\",\n",
    "        is_plagiarized = result.is_plagiarized,\n",
    "        reason = result.reason,\n",
    "        evidence_mine = evidence,\n",
    "        evidence_oai = result.evidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1bc6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid RAG\n",
    "\n",
    "def retrieve_dense(code_query, top_k = 5):\n",
    "    vec = embed_code(code_query)\n",
    "    distances, indexes = dense_index.search(vec, top_k)\n",
    "\n",
    "    out = []\n",
    "    for dist, idx in zip(distances[0], indexes[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        out.append(\n",
    "            {\n",
    "                \"index\": int(idx),\n",
    "                \"dist\": float(dist),\n",
    "                \"path\": chunk_paths[idx],\n",
    "                \"repo\": chunk_repos[idx],\n",
    "                \"text\": chunk_texts[idx]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "def retrieve_bm25(code_query, top_k = 5):\n",
    "    toks = tokenize_code(code_query)\n",
    "    scores = bm25.get_scores(toks)\n",
    "    idxs = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    out = []\n",
    "    for i in idxs:\n",
    "        out.append(\n",
    "            {\n",
    "                \"index\": int(i),\n",
    "                \"score\": float(scores[i]),\n",
    "                \"path\": bm25_chunks[i][\"source_path\"],\n",
    "                \"repo\": bm25_chunks[i][\"repo\"],\n",
    "                \"text\": bm25_chunks[i][\"text\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "def detect_hybrid_rag(code_query, top_k_dense = 5, top_k_bm25 = 5, top_k_fused = 5, w_dense = 0.5):\n",
    "    top_k_fused = min(top_k_fused, top_k_dense + top_k_bm25)\n",
    "    \n",
    "    # retrieve from both methods\n",
    "    dense_hits = retrieve_dense(code_query, top_k = top_k_dense)\n",
    "    bm25_hits = retrieve_bm25(code_query, top_k = top_k_bm25)\n",
    "\n",
    "    # normalize dense scores to 0..1\n",
    "    dense_scores = {}\n",
    "    dists = [h[\"dist\"] for h in dense_hits]\n",
    "    d_max = max(dists)\n",
    "    d_min = min(dists)\n",
    "\n",
    "    for h in dense_hits:\n",
    "        if d_max == d_min:\n",
    "            s = 1.0\n",
    "        else:\n",
    "            s = 1.0 - (h[\"dist\"] - d_min) / (d_max - d_min)\n",
    "        dense_scores[h[\"index\"]] = s\n",
    "\n",
    "    # normalize bm25 scores to 0..1\n",
    "    bm25_scores = {}\n",
    "    b_max = max(h[\"score\"] for h in bm25_hits)\n",
    "\n",
    "    for h in bm25_hits:\n",
    "        s = h[\"score\"] / b_max if b_max > 0 else 0.0\n",
    "        bm25_scores[h[\"index\"]] = s\n",
    "\n",
    "    # fuse based on weight\n",
    "    fused = {}\n",
    "\n",
    "    for idx, s in dense_scores.items():\n",
    "        fused[idx] = fused.get(idx, 0.0) + w_dense * s\n",
    "\n",
    "    for idx, s in bm25_scores.items():\n",
    "        fused[idx] = fused.get(idx, 0.0) + (1.0 - w_dense) * s\n",
    "\n",
    "    # sort fused scores and take top 'top_k_fused'\n",
    "    fused_sorted = sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [idx for idx, _ in fused_sorted[:top_k_fused]]\n",
    "\n",
    "    corpus_snippets = []\n",
    "    evidence = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        text = chunk_texts[idx]\n",
    "        path = chunk_paths[idx]\n",
    "        repo = chunk_repos[idx]\n",
    "\n",
    "        corpus_snippets.append(text)\n",
    "\n",
    "        evidence.append(\n",
    "            {\n",
    "                \"index\": int(idx),\n",
    "                \"repo\": repo,\n",
    "                \"path\": path,\n",
    "                \"dense_score\": dense_scores.get(idx),\n",
    "                \"bm25_score\": bm25_scores.get(idx),\n",
    "                \"fused_score\": fused.get(idx)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    prompt = build_prompt(corpus_snippets, code_query)\n",
    "    result = llm_call(prompt)\n",
    "\n",
    "    return DetectionResult(\n",
    "        method = \"hybrid_rag\",\n",
    "        is_plagiarized = result.is_plagiarized,\n",
    "        reason = result.reason,\n",
    "        evidence_mine = evidence,\n",
    "        evidence_oai = result.evidence\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
