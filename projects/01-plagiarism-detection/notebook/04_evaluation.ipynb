{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare paths\n",
    "\n",
    "root_dir = Path(\"..\")\n",
    "\n",
    "results_dir = root_dir / 'results'\n",
    "predictions_dir = results_dir / 'predictions'\n",
    "\n",
    "summary_csv_path = results_dir / 'summary.csv'\n",
    "dataset_path = root_dir / 'notebook' / 'data' / 'test_dataset.json'\n",
    "chart_path = results_dir / 'comparison_chart.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dadbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all methods into memory\n",
    "%run ./03_interactive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67194f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local methods and parameters for easier handling\n",
    "\n",
    "def call_embedding(code: str, top_k = 10):\n",
    "    return detect_embedding(code, top_k = top_k)\n",
    "\n",
    "def call_llm(code, top_n = 25):\n",
    "    return detect_llm(code, top_n = top_n)\n",
    "\n",
    "def call_rag(code: str, top_k = 5):\n",
    "    return detect_rag(code, top_k = top_k)\n",
    "\n",
    "def call_hybrid_rag(code, top_k_dense = 5, top_k_bm25 = 5, top_k_fused = 5, w_dense = 0.5):\n",
    "    return detect_hybrid_rag(\n",
    "        code,\n",
    "        top_k_dense = top_k_dense,\n",
    "        top_k_bm25 = top_k_bm25,\n",
    "        top_k_fused = top_k_fused,\n",
    "        w_dense = w_dense\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af07f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "@dataclass\n",
    "class CodeSample:\n",
    "    id: str\n",
    "    query_code: str\n",
    "    is_positive: bool\n",
    "    source_hint: str\n",
    "    notes: str\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    return [CodeSample(**item) for item in data]\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c193751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run method with specific parameters on dataset\n",
    "\n",
    "@dataclass\n",
    "class EvaluationRow:\n",
    "    method: str\n",
    "    config_name: str\n",
    "    id: str\n",
    "    is_positive: bool\n",
    "    is_plagiarized: bool\n",
    "    reason: str\n",
    "    evidence_mine: any\n",
    "    evidence_oai: any\n",
    "    ms_elapsed: float\n",
    "    top_k: float = None\n",
    "    top_n: float = None\n",
    "    top_k_dense: float = None\n",
    "    top_k_bm25: float = None\n",
    "    top_k_fused: float = None\n",
    "    w_dense: float = None\n",
    "\n",
    "def run_evaluation_config(method_name, func, params, dataset):\n",
    "    rows = []\n",
    "\n",
    "    if params:\n",
    "        parts = []\n",
    "        for k, v in params.items():\n",
    "            val = str(v).replace(\".\", \"_\")\n",
    "            parts.append(f\"{k}{val}\")\n",
    "        config_str = \"_\".join(parts)\n",
    "    else:\n",
    "        config_str = \"noparams\"\n",
    "\n",
    "    for sample in dataset:\n",
    "        start_time = time.time()\n",
    "        result = func(sample.query_code, **params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        row = EvaluationRow(\n",
    "            method = method_name,\n",
    "            config_name = f\"{method_name}_{config_str}\",\n",
    "            id = sample.id,\n",
    "            is_positive = sample.is_positive,\n",
    "            is_plagiarized = result.is_plagiarized,\n",
    "            reason = result.reason,\n",
    "            evidence_mine = result.evidence_mine,\n",
    "            evidence_oai = result.evidence_oai,\n",
    "            ms_elapsed = (end_time - start_time) * 1000,\n",
    "            top_k = params.get(\"top_k\", -1),\n",
    "            top_n = params.get(\"top_n\", -1),\n",
    "            top_k_dense = params.get(\"top_k_dense\", -1),\n",
    "            top_k_bm25 = params.get(\"top_k_bm25\", -1),\n",
    "            top_k_fused = params.get(\"top_k_fused\", -1),\n",
    "            w_dense = params.get(\"w_dense\", -1)\n",
    "        )\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    print(f\"{method_name} with configuration {config_str} finished\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db61739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save results\n",
    "\n",
    "def save_config_results(config_name, rows):\n",
    "    df = pd.DataFrame([asdict(r) for r in rows])\n",
    "    file_path = predictions_dir / f\"{config_name}.csv\"\n",
    "    df.to_csv(file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb00c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare methods and parameter combinations\n",
    "\n",
    "methods = {\n",
    "    \"pure_embedding\": call_embedding,\n",
    "    \"direct_llm\": call_llm,\n",
    "    \"rag\": call_rag,\n",
    "    \"hybrid_rag\": call_hybrid_rag\n",
    "}\n",
    "\n",
    "embedding_param_grid = [\n",
    "    {\"top_k\": 1},\n",
    "    {\"top_k\": 3},\n",
    "    {\"top_k\": 5},\n",
    "    {\"top_k\": 12}\n",
    "]\n",
    "\n",
    "direct_llm_param_grid = [\n",
    "    {\"top_n\": 5},\n",
    "    {\"top_n\": 10},\n",
    "    {\"top_n\": 20}\n",
    "]\n",
    "\n",
    "rag_param_grid = [\n",
    "    {\"top_k\": 1},\n",
    "    {\"top_k\": 3},\n",
    "    {\"top_k\": 5},\n",
    "    {\"top_k\": 12}\n",
    "]\n",
    "\n",
    "# for hybrid RAG, no big point in varying top_k_dense and top_k_bm25\n",
    "# weights and top_k_fused are more important so we'll only vary them\n",
    "hybrid_rag_param_grid = []\n",
    "\n",
    "for w in [0.25, 0.5, 0.75]:\n",
    "    for k_fused in [5, 10]:\n",
    "        hybrid_rag_param_grid.append(\n",
    "            {\n",
    "                \"top_k_dense\": 5,\n",
    "                \"top_k_bm25\": 5,\n",
    "                \"top_k_fused\": k_fused,\n",
    "                \"w_dense\": w\n",
    "            }\n",
    "        )\n",
    "\n",
    "param_grids = {\n",
    "    \"pure_embedding\": embedding_param_grid,\n",
    "    \"direct_llm\": direct_llm_param_grid,\n",
    "    \"rag\": rag_param_grid,\n",
    "    \"hybrid_rag\": hybrid_rag_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1879ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure_embedding with configuration top_k1 finished\n",
      "pure_embedding with configuration top_k3 finished\n",
      "pure_embedding with configuration top_k5 finished\n",
      "pure_embedding with configuration top_k12 finished\n",
      "direct_llm with configuration top_n5 finished\n",
      "direct_llm with configuration top_n10 finished\n",
      "direct_llm with configuration top_n20 finished\n",
      "rag with configuration top_k1 finished\n",
      "rag with configuration top_k3 finished\n",
      "rag with configuration top_k5 finished\n",
      "rag with configuration top_k12 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused5_w_dense0_25 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused10_w_dense0_25 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused5_w_dense0_5 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused10_w_dense0_5 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused5_w_dense0_75 finished\n",
      "hybrid_rag with configuration top_k_dense5_top_k_bm255_top_k_fused10_w_dense0_75 finished\n",
      "All ablation configurations finished and saved\n"
     ]
    }
   ],
   "source": [
    "all_ablation_rows = []\n",
    "\n",
    "for method_name, func in methods.items():\n",
    "    grid = param_grids.get(method_name, [{}])\n",
    "\n",
    "    for params in grid:\n",
    "        rows = run_evaluation_config(\n",
    "            method_name = method_name,\n",
    "            func = func,\n",
    "            params = params,\n",
    "            dataset = dataset\n",
    "        )\n",
    "        \n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        config_name = rows[0].config_name\n",
    "        save_config_results(config_name, rows)\n",
    "        all_ablation_rows.extend(rows)\n",
    "    print()\n",
    "\n",
    "ablations_df = pd.DataFrame([asdict(r) for r in all_ablation_rows])\n",
    "ablations_path = predictions_dir / \"ablations_all.csv\"\n",
    "ablations_df.to_csv(ablations_path, index = False)\n",
    "\n",
    "print(\"All ablation configurations finished and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to evaluate results using metrics\n",
    "\n",
    "def confusion_counts(dataframe):\n",
    "    tp = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == True)).sum())\n",
    "    fp = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == True)).sum())\n",
    "    tn = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == False)).sum())\n",
    "    fn = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == False)).sum())\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def calculate_metrics(true_positive, false_positive, true_negative, false_negative):\n",
    "    # precision = how often a detected plagiarism case was really a plagiarism\n",
    "    assumed_plagiarized_cnt = true_positive + false_positive\n",
    "    precision = true_positive / assumed_plagiarized_cnt if assumed_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    # recall = how many real plagiarism cases the model caught\n",
    "    total_plagiarized_cnt = true_positive + false_negative\n",
    "    recall = true_positive / total_plagiarized_cnt if total_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (true_positive + true_negative) / max(true_positive + false_positive + true_negative + false_negative, 1)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00683d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['method', 'config_name', 'id', 'is_positive', 'is_plagiarized',\n",
      "       'reason', 'evidence_mine', 'evidence_oai', 'ms_elapsed'],\n",
      "      dtype='object')\n",
      "             config_name          method\n",
      "0  pure_embedding_top_k1  pure_embedding\n",
      "1  pure_embedding_top_k1  pure_embedding\n",
      "2  pure_embedding_top_k1  pure_embedding\n",
      "3  pure_embedding_top_k1  pure_embedding\n",
      "4  pure_embedding_top_k1  pure_embedding\n",
      "no top_k column\n"
     ]
    }
   ],
   "source": [
    "print(ablations_df.columns)\n",
    "\n",
    "print(ablations_df[[\"config_name\", \"method\"]].head())\n",
    "\n",
    "# Check one param directly\n",
    "print(ablations_df[\"top_k\"].unique() if \"top_k\" in ablations_df.columns else \"no top_k column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ddc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "for config_name, df in ablations_df.groupby(\"config_name\"):\n",
    "    method_name = df[\"method\"].iloc[0]\n",
    "\n",
    "    tp, fp, tn, fn = confusion_counts(df)\n",
    "    scores = calculate_metrics(tp, fp, tn, fn)\n",
    "    avg_ms = float(df[\"ms_elapsed\"].mean()) if len(df) else 0.0\n",
    "\n",
    "    def get_param(col):\n",
    "        return df[col].dropna().iloc[0] if col in df.columns and df[col].notna().any() else None\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"method\": method_name,\n",
    "        \"config_name\": config_name,\n",
    "        \"n\": int(len(df)),\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1\": scores[\"f1\"],\n",
    "        \"accuracy\": scores[\"accuracy\"],\n",
    "        \"avg_ms\": avg_ms,\n",
    "        \"top_k\": get_param(\"top_k\"),\n",
    "        \"top_n\": get_param(\"top_n\"),\n",
    "        \"top_k_dense\": get_param(\"top_k_dense\"),\n",
    "        \"top_k_bm25\": get_param(\"top_k_bm25\"),\n",
    "        \"top_k_fused\": get_param(\"top_k_fused\"),\n",
    "        \"w_dense\": get_param(\"w_dense\")\n",
    "    })\n",
    "\n",
    "summary = (\n",
    "    pd.DataFrame(summary_rows)\n",
    "    .sort_values([\"method\", \"f1\"], ascending = [True, False])\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "\n",
    "summary.to_csv(summary_csv_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
