{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b7539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare paths\n",
    "\n",
    "root_path = Path(\"..\")\n",
    "results_dir = root_path / 'results'\n",
    "dataset_path = root_path / 'notebook' / 'data' / 'test_dataset.json'\n",
    "\n",
    "predictions_csv_path = results_dir / 'predictions.csv'\n",
    "summary_csv_path = results_dir / 'summary.csv'\n",
    "ablations_csv_path = results_dir / 'ablations.csv'\n",
    "chart_path = results_dir / 'comparison_chart.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ./03_interactive.ipynb to load all methods into memory\n",
    "%run ./03_interactive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67194f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local methods and parameters for easier handling\n",
    "\n",
    "def call_embedding(code: str, top_k = 10):\n",
    "    return detect_embedding(code, top_k = top_k)\n",
    "\n",
    "def call_llm(code, top_n = 25):\n",
    "    return detect_llm(code, top_n = top_n)\n",
    "\n",
    "def call_rag(code: str, top_k = 5):\n",
    "    return detect_rag(code, top_k = top_k)\n",
    "\n",
    "def call_hybrid_rag(code, top_k_dense = 5, top_k_bm25 = 5, top_k_fused = 5, w_dense = 0.5):\n",
    "    return detect_hybrid_rag(\n",
    "        code,\n",
    "        top_k_dense = top_k_dense,\n",
    "        top_k_bm25 = top_k_bm25,\n",
    "        top_k_fused = top_k_fused,\n",
    "        w_dense = w_dense\n",
    "    )\n",
    "\n",
    "methods = {\n",
    "    \"pure_embedding\": call_embedding,\n",
    "    \"direct_llm\": call_llm,\n",
    "    \"rag\": call_rag,\n",
    "    \"hybrid_rag\": call_hybrid_rag\n",
    "}\n",
    "\n",
    "method_params = {\n",
    "    \"pure_embedding\": {\"top_k\": 10},\n",
    "    \"direct_llm\": {\"top_n\": 25},\n",
    "    \"rag\": {\"top_k\": 5},\n",
    "    \"hybrid_rag\": {\"top_k_dense\": 5, \"top_k_bm25\": 5, \"top_k_fused\": 5, \"w_dense\": 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af07f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "@dataclass\n",
    "class CodeSample:\n",
    "    id: str\n",
    "    query_code: str\n",
    "    is_positive: bool\n",
    "    source_hint: str\n",
    "    notes: str\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    return [CodeSample(**item) for item in data]\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c193751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure_embedding finished\n",
      "direct_llm finished\n",
      "direct_llm finished\n",
      "rag finished\n",
      "rag finished\n",
      "hybrid_rag finished\n",
      "hybrid_rag finished\n"
     ]
    }
   ],
   "source": [
    "# run on dataset and create results\n",
    "\n",
    "@dataclass\n",
    "class EvaluationRow:\n",
    "    id: str\n",
    "    is_positive: bool\n",
    "    method: str\n",
    "    is_plagiarized: bool\n",
    "    reason: str\n",
    "    evidence_mine: any\n",
    "    evidence_oai: any\n",
    "    ms_elapsed: float\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, func in methods.items():\n",
    "    params = method_params.get(name, {})\n",
    "    \n",
    "    for sample in dataset:\n",
    "        start_time = time.time()\n",
    "        result = func(sample.query_code, **params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        row = EvaluationRow(\n",
    "            id = sample.id,\n",
    "            is_positive = sample.is_positive,\n",
    "            method = result.method,\n",
    "            is_plagiarized = result.is_plagiarized,\n",
    "            reason = result.reason,\n",
    "            evidence_mine = result.evidence_mine,\n",
    "            evidence_oai = result.evidence_oai,\n",
    "            ms_elapsed = (end_time - start_time) * 1000\n",
    "        )\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    print(f\"{name} finished\")\n",
    "\n",
    "# convert and save\n",
    "results = pd.DataFrame([asdict(r) for r in rows])\n",
    "results.to_csv(predictions_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "957e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results using metrics and form summary\n",
    "\n",
    "def confusion_counts(dataframe):\n",
    "    tp = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == True)).sum())\n",
    "    fp = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == True)).sum())\n",
    "    tn = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == False)).sum())\n",
    "    fn = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == False)).sum())\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def calculate_metrics(true_positive, false_positive, true_negative, false_negative):\n",
    "    # precision = how often a detected plagiarism case was really a plagiarism\n",
    "    assumed_plagiarized_cnt = true_positive + false_positive\n",
    "    precision = true_positive / assumed_plagiarized_cnt if assumed_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    # recall = how many real plagiarism cases the model caught\n",
    "    total_plagiarized_cnt = true_positive + false_negative\n",
    "    recall = true_positive / total_plagiarized_cnt if total_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (true_positive + true_negative) / max(true_positive + false_positive + true_negative + false_negative, 1)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "summary_rows = []\n",
    "for method_name, dataframe in results.groupby(\"method\"):\n",
    "    tp, fp, tn, fn = confusion_counts(dataframe)\n",
    "    scores = calculate_metrics(tp, fp, tn, fn)\n",
    "    avg_ms = float(dataframe[\"ms_elapsed\"].mean()) if len(dataframe) else 0.0\n",
    "    \n",
    "    summary_rows.append({\n",
    "        \"method\": method_name,\n",
    "        \"n\": int(len(dataframe)),\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1\": scores[\"f1\"],\n",
    "        \"accuracy\": scores[\"accuracy\"],\n",
    "        \"avg_ms\": avg_ms\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(summary_rows).sort_values(\"f1\", ascending = False).reset_index(drop = True)\n",
    "summary.to_csv(summary_csv_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
