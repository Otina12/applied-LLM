{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b7539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare paths\n",
    "\n",
    "root_dir = Path(\"..\")\n",
    "\n",
    "results_dir = root_dir / 'results'\n",
    "predictions_dir = results_dir / 'predictions'\n",
    "\n",
    "summary_csv_path = results_dir / 'summary.csv'\n",
    "dataset_path = root_dir / 'notebook' / 'data' / 'test_dataset.json'\n",
    "chart_path = results_dir / 'comparison_chart.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dadbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all 4 methods into memory\n",
    "%run ./03_interactive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67194f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local methods and parameters for easier handling\n",
    "\n",
    "def call_embedding(code: str, top_k = 10):\n",
    "    return detect_embedding(code, top_k = top_k)\n",
    "\n",
    "def call_llm(code, top_n = 25):\n",
    "    return detect_llm(code, top_n = top_n)\n",
    "\n",
    "def call_rag(code: str, top_k = 5):\n",
    "    return detect_rag(code, top_k = top_k)\n",
    "\n",
    "def call_hybrid_rag(code, top_k_dense = 5, top_k_bm25 = 5, top_k_fused = 5, w_dense = 0.5):\n",
    "    return detect_hybrid_rag(\n",
    "        code,\n",
    "        top_k_dense = top_k_dense,\n",
    "        top_k_bm25 = top_k_bm25,\n",
    "        top_k_fused = top_k_fused,\n",
    "        w_dense = w_dense\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af07f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "@dataclass\n",
    "class CodeSample:\n",
    "    id: str\n",
    "    query_code: str\n",
    "    is_positive: bool\n",
    "    source_hint: str\n",
    "    notes: str\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    return [CodeSample(**item) for item in data]\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c84a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare methods and parameter combinations\n",
    "\n",
    "methods = {\n",
    "    \"pure_embedding\": call_embedding,\n",
    "    \"direct_llm\": call_llm,\n",
    "    \"rag\": call_rag,\n",
    "    \"hybrid_rag\": call_hybrid_rag\n",
    "}\n",
    "\n",
    "embedding_param_grid = [\n",
    "    {\"top_k\": 3},\n",
    "    {\"top_k\": 5},\n",
    "    {\"top_k\": 10},\n",
    "    {\"top_k\": 20},\n",
    "    {\"top_k\": 50}\n",
    "]\n",
    "\n",
    "direct_llm_param_grid = [\n",
    "    {\"top_n\": 5},\n",
    "    {\"top_n\": 10},\n",
    "    {\"top_n\": 25},\n",
    "    {\"top_n\": 50}\n",
    "]\n",
    "\n",
    "rag_param_grid = [\n",
    "    {\"top_k\": 1},\n",
    "    {\"top_k\": 5},\n",
    "    {\"top_k\": 10}\n",
    "]\n",
    "\n",
    "# for hybrid RAG:\n",
    "# weights and top_k_fused are more important and interesting so I'll only vary them\n",
    "# or else I'll run out of money :)\n",
    "hybrid_rag_param_grid = []\n",
    "\n",
    "# 5 * 3 = 15 hybrid_rag cases in total\n",
    "for w in [0.2, 0.5, 0.8]:\n",
    "    for k_fused in [1, 5, 10]:\n",
    "        hybrid_rag_param_grid.append(\n",
    "            {\n",
    "                \"top_k_dense\": 5,\n",
    "                \"top_k_bm25\": 5,\n",
    "                \"top_k_fused\": k_fused,\n",
    "                \"w_dense\": w\n",
    "            }\n",
    "        )\n",
    "\n",
    "param_grids = {\n",
    "    \"pure_embedding\": embedding_param_grid,\n",
    "    \"direct_llm\": direct_llm_param_grid,\n",
    "    \"rag\": rag_param_grid,\n",
    "    \"hybrid_rag\": hybrid_rag_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c193751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run method with specific parameters on dataset\n",
    "\n",
    "@dataclass\n",
    "class EvaluationRow:\n",
    "    method: str\n",
    "    config_name: str\n",
    "    id: str\n",
    "    is_positive: bool\n",
    "    is_plagiarized: bool\n",
    "    reason: str\n",
    "    evidence_mine: any\n",
    "    evidence_oai: any\n",
    "    ms_elapsed: float\n",
    "\n",
    "def run_evaluation_config(method_name, params, dataset):\n",
    "    rows = []\n",
    "    func = methods[method_name]\n",
    "\n",
    "    if params:\n",
    "        parts = []\n",
    "        for k, v in params.items():\n",
    "            val = str(v).replace(\".\", \"_\")\n",
    "            parts.append(f\"{k}-{val}\")\n",
    "        config_str = \"_\".join(parts)\n",
    "    else:\n",
    "        config_str = \"noparams\"\n",
    "\n",
    "    for sample in dataset:\n",
    "        start_time = time.time()\n",
    "        result = func(sample.query_code, **params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        row = EvaluationRow(\n",
    "            method = method_name,\n",
    "            config_name = config_str,\n",
    "            id = sample.id,\n",
    "            is_positive = sample.is_positive,\n",
    "            is_plagiarized = result.is_plagiarized,\n",
    "            reason = result.reason,\n",
    "            evidence_mine = result.evidence_mine,\n",
    "            evidence_oai = result.evidence_oai,\n",
    "            ms_elapsed = (end_time - start_time) * 1000\n",
    "        )\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    print(f\"{method_name} with configuration {config_str} finished\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db61739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save results\n",
    "\n",
    "def save_config_results(method_name, config_name, rows):\n",
    "    df = pd.DataFrame([asdict(r) for r in rows])\n",
    "    file_path = predictions_dir / method_name / f\"{config_name}.csv\"\n",
    "    file_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "    df.to_csv(file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1879ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure_embedding with configuration top_k-20 finished\n",
      "pure_embedding with configuration top_k-3 finished\n",
      "pure_embedding with configuration top_k-5 finished\n",
      "pure_embedding with configuration top_k-10 finished\n",
      "pure_embedding with configuration top_k-50 finished\n",
      "direct_llm with configuration top_n-25 finished\n",
      "direct_llm with configuration top_n-5 finished\n",
      "direct_llm with configuration top_n-10 finished\n",
      "direct_llm with configuration top_n-50 finished\n",
      "rag with configuration top_k-5 finished\n",
      "rag with configuration top_k-1 finished\n",
      "rag with configuration top_k-10 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-1_w_dense-0_2 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-5_w_dense-0_2 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-10_w_dense-0_2 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-1_w_dense-0_5 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-5_w_dense-0_5 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-10_w_dense-0_5 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-1_w_dense-0_8 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-5_w_dense-0_8 finished\n",
      "hybrid_rag with configuration top_k_dense-5_top_k_bm25-5_top_k_fused-10_w_dense-0_8 finished\n",
      "All ablation configurations finished and saved\n"
     ]
    }
   ],
   "source": [
    "# run all methods with all parameters combinations\n",
    "# this notebook segment alone was taking 15+ minutes, so I made it parallel\n",
    "\n",
    "all_ablation_rows = []\n",
    "tasks = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "    for method_name, func in methods.items():\n",
    "        grid = param_grids.get(method_name, [{}])\n",
    "\n",
    "        for params in grid:\n",
    "            future = executor.submit(run_evaluation_config, method_name, params, dataset)\n",
    "            tasks.append(future)\n",
    "\n",
    "    for future in as_completed(tasks):\n",
    "        rows = future.result()\n",
    "        if not rows:\n",
    "            continue\n",
    "        \n",
    "        method_name_thread_safe = rows[0].method\n",
    "        config_name = rows[0].config_name\n",
    "        save_config_results(method_name_thread_safe, config_name, rows)\n",
    "        all_ablation_rows.extend(rows)\n",
    "\n",
    "ablations_df = pd.DataFrame([asdict(r) for r in all_ablation_rows])\n",
    "ablations_path = predictions_dir / \"ablations_all.csv\"\n",
    "ablations_df.to_csv(ablations_path, index = False)\n",
    "\n",
    "print(\"All ablation configurations finished and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to evaluate results using metrics\n",
    "\n",
    "def confusion_counts(dataframe):\n",
    "    tp = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == True)).sum())\n",
    "    fp = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == True)).sum())\n",
    "    tn = int(((dataframe.is_positive == False) & (dataframe.is_plagiarized == False)).sum())\n",
    "    fn = int(((dataframe.is_positive == True) & (dataframe.is_plagiarized == False)).sum())\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def calculate_metrics(true_positive, false_positive, true_negative, false_negative):\n",
    "    # precision = how often a detected plagiarism case was really a plagiarism\n",
    "    assumed_plagiarized_cnt = true_positive + false_positive\n",
    "    precision = true_positive / assumed_plagiarized_cnt if assumed_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    # recall = how many real plagiarism cases the model caught\n",
    "    total_plagiarized_cnt = true_positive + false_negative\n",
    "    recall = true_positive / total_plagiarized_cnt if total_plagiarized_cnt > 0 else 0.0\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (true_positive + true_negative) / max(true_positive + false_positive + true_negative + false_negative, 1)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ddc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "grouped_ablations = ablations_df.groupby(\"config_name\")\n",
    "\n",
    "for (method_name, config_name), df in ablations_df.groupby([\"method\", \"config_name\"]):\n",
    "    tp, fp, tn, fn = confusion_counts(df)\n",
    "    scores = calculate_metrics(tp, fp, tn, fn)\n",
    "    avg_ms = float(df[\"ms_elapsed\"].mean()) if len(df) else 0.0\n",
    "\n",
    "    def get_param(col):\n",
    "        return df[col].dropna().iloc[0] if col in df.columns and df[col].notna().any() else None\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"method\": method_name,\n",
    "        \"config_name\": config_name,\n",
    "        \"n\": int(len(df)),\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1\": scores[\"f1\"],\n",
    "        \"accuracy\": scores[\"accuracy\"],\n",
    "        \"avg_ms\": avg_ms\n",
    "    })\n",
    "\n",
    "summary = (\n",
    "    pd.DataFrame(summary_rows)\n",
    "    .assign(method = pd.Categorical( # keep same order as in code for structure\n",
    "        pd.DataFrame(summary_rows)[\"method\"],   \n",
    "        categories = methods,\n",
    "        ordered = True\n",
    "    ))\n",
    "    .sort_values([\"method\", \"f1\", \"avg_ms\"], ascending = [True, True, True])\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "\n",
    "summary.to_csv(summary_csv_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
